{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Bernoulli, Categorical\n",
    "from torchtext import datasets\n",
    "import os\n",
    "import time\n",
    "import numpy as np \n",
    "import random\n",
    "import argparse\n",
    "\n",
    "from networks import Distilbert_LSTM, Policy_C, Policy_N, Policy_S, ValueNetwork\n",
    "from utils.utils import sample_policy_c, sample_policy_n, sample_policy_s, evaluate_lm, compute_policy_value_losses\n",
    "from utils.utils import cnn_cost, clstm_cost, c_cost, n_cost, s_cost, openDfFromPickle, calculate_stats_from_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Number of training examples: 20000\n",
      "Number of validation examples: 5000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print('Reading data...')\n",
    "train_data = openDfFromPickle(\"C:\\\\Users\\\\mrbal\\\\Documents\\\\NLP\\\\RL\\\\basic_reinforcement_learning\\\\NLP_datasets\\\\imdb\\\\imdb_train_distilbert-base-uncased.pkl\")\n",
    "valid_data = openDfFromPickle(\"C:\\\\Users\\\\mrbal\\\\Documents\\\\NLP\\\\RL\\\\basic_reinforcement_learning\\\\NLP_datasets\\\\imdb\\\\imdb_val_distilbert-base-uncased.pkl\")\n",
    "test_data = openDfFromPickle(\"C:\\\\Users\\\\mrbal\\\\Documents\\\\NLP\\\\RL\\\\basic_reinforcement_learning\\\\NLP_datasets\\\\imdb\\\\imdb_test_distilbert-base-uncased.pkl\")\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_str</th>\n",
       "      <th>text_bert_input_ids</th>\n",
       "      <th>text_bert_attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23311</th>\n",
       "      <td>I borrowed this movie despite its extremely lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[101, 1045, 11780, 2023, 3185, 2750, 2049, 518...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23623</th>\n",
       "      <td>After the unexpected accident that killed an i...</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[101, 2044, 1996, 9223, 4926, 2008, 2730, 2019...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>On the summer blockbuster hit BASEketball This...</td>\n",
       "      <td>0</td>\n",
       "      <td>neg</td>\n",
       "      <td>[101, 2006, 1996, 2621, 27858, 2718, 2918, 348...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12645</th>\n",
       "      <td>Can Scarcely Imagine Better Movie Than This br...</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[101, 2064, 20071, 5674, 2488, 3185, 2084, 202...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>A still famous but decadent actor Morgan Freem...</td>\n",
       "      <td>0</td>\n",
       "      <td>neg</td>\n",
       "      <td>[101, 1037, 2145, 3297, 2021, 5476, 3372, 3364...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21575</th>\n",
       "      <td>My discovery of the cinema of Jan Svankmajer o...</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[101, 2026, 5456, 1997, 1996, 5988, 1997, 5553...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>The story is similar to ET an extraterrestrial...</td>\n",
       "      <td>0</td>\n",
       "      <td>neg</td>\n",
       "      <td>[101, 1996, 2466, 2003, 2714, 2000, 3802, 2019...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>I have read the novel Reaper of Ben Mezrich fe...</td>\n",
       "      <td>0</td>\n",
       "      <td>neg</td>\n",
       "      <td>[101, 1045, 2031, 3191, 1996, 3117, 19559, 199...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>Went to see this finnish film and ve got to sa...</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[101, 2253, 2000, 2156, 2023, 6983, 2143, 1998...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23654</th>\n",
       "      <td>I first saw Breaking Glass in and thought that...</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[101, 1045, 2034, 2387, 4911, 3221, 1999, 1998...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label label_str  \\\n",
       "23311  I borrowed this movie despite its extremely lo...      1       pos   \n",
       "23623  After the unexpected accident that killed an i...      1       pos   \n",
       "1020   On the summer blockbuster hit BASEketball This...      0       neg   \n",
       "12645  Can Scarcely Imagine Better Movie Than This br...      1       pos   \n",
       "1533   A still famous but decadent actor Morgan Freem...      0       neg   \n",
       "...                                                  ...    ...       ...   \n",
       "21575  My discovery of the cinema of Jan Svankmajer o...      1       pos   \n",
       "5390   The story is similar to ET an extraterrestrial...      0       neg   \n",
       "860    I have read the novel Reaper of Ben Mezrich fe...      0       neg   \n",
       "15795  Went to see this finnish film and ve got to sa...      1       pos   \n",
       "23654  I first saw Breaking Glass in and thought that...      1       pos   \n",
       "\n",
       "                                     text_bert_input_ids  \\\n",
       "23311  [101, 1045, 11780, 2023, 3185, 2750, 2049, 518...   \n",
       "23623  [101, 2044, 1996, 9223, 4926, 2008, 2730, 2019...   \n",
       "1020   [101, 2006, 1996, 2621, 27858, 2718, 2918, 348...   \n",
       "12645  [101, 2064, 20071, 5674, 2488, 3185, 2084, 202...   \n",
       "1533   [101, 1037, 2145, 3297, 2021, 5476, 3372, 3364...   \n",
       "...                                                  ...   \n",
       "21575  [101, 2026, 5456, 1997, 1996, 5988, 1997, 5553...   \n",
       "5390   [101, 1996, 2466, 2003, 2714, 2000, 3802, 2019...   \n",
       "860    [101, 1045, 2031, 3191, 1996, 3117, 19559, 199...   \n",
       "15795  [101, 2253, 2000, 2156, 2023, 6983, 2143, 1998...   \n",
       "23654  [101, 1045, 2034, 2387, 4911, 3221, 1999, 1998...   \n",
       "\n",
       "                                text_bert_attention_mask  \n",
       "23311  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "23623  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1020   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "12645  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1533   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "...                                                  ...  \n",
       "21575  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "5390   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "860    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "15795  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "23654  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "\n",
       "[20000 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After the unexpected accident that killed an inexperienced climber Michelle Joyner Eight months has passed The Rocky Mountain Rescue receive distress call set by brilliant terrorist mastermind Eric Quaien John Lithgow Quaien has lost three large cases that has millions of dollars inside Two experienced climbers Walker Sylvester Stallone and Tucker Micheal Rooker and helicopter pilot Janine Turner are to the rescue but they are set by trap by Quaien and his men Now the two climbers and pilot are forced to play deadly game of hide and seek While Quaien is trying to find the millions of dollars and he kidnapped Tucker to find the money Once Tucker finds the money Tucker will be dead Against explosive firepower bitter cold and dizzying heights Walker must outwit Quaien for survival br br Directed by Renny Harlin Driven Mindhunters Nightmare on Elm Street The Dream Master made an entertaining non stop action picture This film is spectacular exciting visually exciting action picture with plenty of dark humour as well This was one of the biggest hits of This is one of Harlin best film Lithgow is terrific entertaining villain Stallone certainly made an short comeback of this sharp thriller This is probably Harlin best work as filmmaker br br DVD has an sharp anamorphic Widescreen transfer and an terrific Dolby Digital Surround Sound DVD has an running commentary track by the director with comments by Stallone DVD also has technical crew commentary as well DVD has behind the scenes featurette two deleted scenes with introduction by the director and more Do not miss this great action film Screenplay by Micheal France Fantastic Four and actor Stallone The Rocky Series Based on premise by John Long Excellent Cinematography by Alex Thomson S Alien Demolition Man Legend Oscar Nominated for Best Sound Best Sound Editing and Best Visual Effects Panavision '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"text\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# split the datasets into batches\n",
    "BATCH_SIZE = 1  # the batch size for a dataset iterator\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20000, 400]) torch.Size([20000])\n",
      "torch.Size([5000, 400]) torch.Size([5000])\n",
      "torch.Size([25000, 400]) torch.Size([25000])\n"
     ]
    }
   ],
   "source": [
    "xtrain = torch.from_numpy(np.stack(train_data[\"text_bert_input_ids\"].values))[:, 0:400]\n",
    "xtrain_mask = torch.from_numpy(np.stack(train_data[\"text_bert_attention_mask\"].values))[:, 0:400]\n",
    "ytrain = torch.from_numpy(train_data[\"label\"].values)\n",
    "xvalid = torch.from_numpy(np.stack(valid_data[\"text_bert_input_ids\"].values))[:, 0:400]\n",
    "xvalid_mask = torch.from_numpy(np.stack(valid_data[\"text_bert_attention_mask\"].values))[:, 0:400]\n",
    "yvalid = torch.from_numpy(valid_data[\"label\"].values)\n",
    "xtest = torch.from_numpy(np.stack(test_data[\"text_bert_input_ids\"].values))[:, 0:400]\n",
    "xtest_mask = torch.from_numpy(np.stack(test_data[\"text_bert_attention_mask\"].values))[:, 0:400]\n",
    "ytest = torch.from_numpy(test_data[\"label\"].values)\n",
    "\n",
    "print(xtrain.shape, ytrain.shape)\n",
    "print(xvalid.shape, yvalid.shape)\n",
    "print(xtest.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "train_loader = DataLoader(TensorDataset(xtrain, xtrain_mask, ytrain), batch_size=BATCH_SIZE)\n",
    "valid_loader = DataLoader(TensorDataset(xvalid, xvalid_mask, yvalid), batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(TensorDataset(xtest, xtest_mask, ytest), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2023)\n",
    "np.random.seed(2023)\n",
    "torch.manual_seed(2023)\n",
    "torch.cuda.manual_seed(2023)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "NUM_RNN_LAYERS = 8\n",
    "HIDDEN_DIM_LSTM = 1024 \n",
    "HIDDEN_DIM_DENSE = 1 * HIDDEN_DIM_LSTM # HIDDEN_DIM_LSTM * NUM_RNN_LAYERS\n",
    "OUTPUT_DIM = 1\n",
    "CHUNCK_SIZE = 20\n",
    "MAX_K = 4  # the output dimension for step size 0, 1, 2, 3\n",
    "LABEL_DIM = 2\n",
    "BATCH_SIZE = 1\n",
    "gamma = 0.99\n",
    "alpha = 0.2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy start: torch.Size([1, 20])\n",
      "Transformer: torch.Size([1, 20, 768])\n",
      "permute: torch.Size([20, 1, 768])\n",
      "dummy start: torch.Size([1, 20])\n",
      "Transformer: torch.Size([1, 20, 768])\n",
      "permute: torch.Size([20, 1, 768])\n",
      "lstm out: torch.Size([20, 1, 1024]), hidden: torch.Size([8, 1, 1024]), cell: torch.Size([8, 1, 1024])\n",
      "torch.Size([8, 1, 1024])\n",
      "Distilbert_LSTM(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (lstm): LSTM(768, 1024, num_layers=8)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# set up the criterion\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "# set up models\n",
    "trns_lstm = Distilbert_LSTM(NUM_RNN_LAYERS, HIDDEN_DIM_LSTM, bert_checkpoint=\"distilbert-base-uncased\").to(device)\n",
    "print(trns_lstm)\n",
    "policy_s = Policy_S(HIDDEN_DIM_DENSE, HIDDEN_DIM_DENSE, OUTPUT_DIM).to(device)\n",
    "policy_n = Policy_N(HIDDEN_DIM_DENSE, HIDDEN_DIM_DENSE, MAX_K).to(device)\n",
    "policy_c = Policy_C(HIDDEN_DIM_DENSE, HIDDEN_DIM_DENSE, LABEL_DIM).to(device)\n",
    "value_net = ValueNetwork(HIDDEN_DIM_DENSE, HIDDEN_DIM_DENSE, OUTPUT_DIM).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up optimiser\n",
    "params_pg = list(policy_s.parameters()) + list(policy_c.parameters()) + list(policy_n.parameters())\n",
    "optim_loss_1 = optim.Adam(trns_lstm.distilbert.parameters(), lr=1e-7)\n",
    "optim_loss_2 = optim.Adam(trns_lstm.lstm.parameters(), lr=learning_rate)\n",
    "optim_policy = optim.Adam(params_pg, lr=learning_rate)\n",
    "optim_value = optim.Adam(value_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode(policy_loss_sum, encoder_loss_sum, baseline_value_batch):\n",
    "    '''\n",
    "    Called when a data sample has been processed.\n",
    "    '''\n",
    "    baseline_value_sum = torch.stack(baseline_value_batch).sum()\n",
    "    policy_loss = torch.stack(policy_loss_sum).mean()\n",
    "    encoder_loss = torch.stack(encoder_loss_sum).mean()\n",
    "    objective_loss = encoder_loss - policy_loss + baseline_value_sum\n",
    "    # set gradient to zero\n",
    "    optim_loss_1.zero_grad()\n",
    "    optim_loss_2.zero_grad()\n",
    "    optim_policy.zero_grad()\n",
    "    optim_value.zero_grad()\n",
    "    # back propagation\n",
    "    objective_loss.backward()\n",
    "    # gradient update\n",
    "    optim_loss_1.step()\n",
    "    optim_loss_2.zero_grad()\n",
    "    optim_policy.step()\n",
    "    optim_value.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2_6_2023', '2_6_2023_12_11')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "now_time = str(now.day) + \"_\" + str(now.month) + \"_\" + str(now.year) + \"_\" + str(now.hour) + \"_\" + str(now.minute)\n",
    "now_date = str(now.day) + \"_\" + str(now.month) + \"_\" + str(now.year)\n",
    "now_date, now_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(comment=f\"distilbert-base_{now_date}__{now_time}_freeze_10_step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trns_lstm.distilbert.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts...\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 3525, accuracy: 0.469, precision: 0.469, recall: 0.469, f1: 0.47: 100%|██████████| 20000/20000 [36:40<00:00,  9.09it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 2200.48 s\n",
      "reread_or_skim_times in this epoch: 3525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [03:06<00:00, 26.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 186.12 s\n",
      "Average FLOPs per sample:  1377540\n",
      "Epoch: 1, Accuracy on the validation set: 0.50\n",
      "\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 2293, accuracy: 0.594, precision: 0.592, recall: 0.586, f1: 0.58: 100%|██████████| 20000/20000 [31:52<00:00, 10.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 1912.16 s\n",
      "reread_or_skim_times in this epoch: 2293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [03:40<00:00, 22.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 220.13 s\n",
      "Average FLOPs per sample:  1428056\n",
      "Epoch: 2, Accuracy on the validation set: 0.48\n",
      "\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 4400, accuracy: 0.625, precision: 0.627, recall: 0.627, f1: 0.62: 100%|██████████| 20000/20000 [31:26<00:00, 10.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 1886.12 s\n",
      "reread_or_skim_times in this epoch: 4400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [03:03<00:00, 27.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 183.84 s\n",
      "Average FLOPs per sample:  1377540\n",
      "Epoch: 3, Accuracy on the validation set: 0.50\n",
      "\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 3871, accuracy: 0.375, precision: 0.376, recall: 0.376, f1: 0.38: 100%|██████████| 20000/20000 [29:45<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 1785.19 s\n",
      "reread_or_skim_times in this epoch: 3871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [03:01<00:00, 27.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 181.53 s\n",
      "Average FLOPs per sample:  1377540\n",
      "Epoch: 4, Accuracy on the validation set: 0.51\n",
      "\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 7032, accuracy: 0.469, precision: 0.458, recall: 0.461, f1: 0.46: 100%|██████████| 20000/20000 [35:10<00:00,  9.48it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 2110.09 s\n",
      "reread_or_skim_times in this epoch: 7032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [03:23<00:00, 24.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 203.97 s\n",
      "Average FLOPs per sample:  1377540\n",
      "Epoch: 5, Accuracy on the validation set: 0.51\n",
      "Compute the accuracy on the testing set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 25000/25000 [16:46<00:00, 24.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 1006.77 s\n",
      "Average FLOPs per sample:  1377540\n",
      "Accuracy on the testing set: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training starts...')\n",
    "\n",
    "for epoch in range(5):\n",
    "    print('\\nEpoch', epoch+1)\n",
    "    # log the start time of the epoch\n",
    "    start = time.time()\n",
    "    # set the models in training mode\n",
    "    trns_lstm.train()\n",
    "    policy_s.train()\n",
    "    policy_n.train()\n",
    "    policy_c.train()\n",
    "    # reset the count of reread_or_skim_times\n",
    "    reread_or_skim_times = 0\n",
    "    policy_loss_sum = []\n",
    "    encoder_loss_sum = []\n",
    "    baseline_value_batch = []\n",
    "    pbar = tqdm(train_loader)\n",
    "    cm = np.zeros((LABEL_DIM, LABEL_DIM))\n",
    "    for index, (x, xmask, y) in enumerate(pbar):\n",
    "        label = y.to(device)\n",
    "        text = x.to(device).view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE) # transform 1*400 to 20*1*20\n",
    "        text_mask = xmask.to(device).view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE)\n",
    "        curr_step = 0  # the position of the current chunk\n",
    "        h_0 = torch.zeros([NUM_RNN_LAYERS,1,HIDDEN_DIM_LSTM]).to(device)  # run on GPU\n",
    "        c_0 = torch.zeros([NUM_RNN_LAYERS,1,HIDDEN_DIM_LSTM]).to(device)\n",
    "        count = 0  # maximum skim/reread time: 5\n",
    "        baseline_value_ep = []\n",
    "        saved_log_probs = []  # for the use of policy gradient update\n",
    "        # collect the computational costs for every time step\n",
    "        cost_ep = []  \n",
    "        while curr_step < CHUNCK_SIZE and count < 10: \n",
    "            # Loop until a text can be classified or currstep is up to 20 or count reach the maximum i.e. 5.\n",
    "            # update count\n",
    "            count += 1\n",
    "            # pass the input through cnn-lstm and policy s\n",
    "            text_input = text[curr_step] # text_input 1*20\n",
    "            text_mask_input = text_mask[curr_step]\n",
    "            # print(f\"input h: {h_0.shape}\")\n",
    "            ht, ct = trns_lstm(text_input, text_mask_input, h_0, c_0)  #ht: NUM_RNN_LAYERS * 1 * HIDDEN_DIM_LSTM\n",
    "            # separate the value which is the input of value net\n",
    "            ht_ = ht.clone().detach().requires_grad_(True)\n",
    "            \n",
    "            # NUM_RNN_LAYERS * 1 * 128, next input of lstm\n",
    "            h_0 = ht # .unsqueeze(0)\n",
    "            c_0 = ct\n",
    "            \n",
    "            ht_ = ht_[-1, :, :]# .view(1, ht_.shape[0] * ht_.shape[2]) # ht_: 1, NUM_RNN_LAYERS * HIDDEN_DIM_LSTM\n",
    "            ht = ht[-1, :, :] # .view(1, ht.shape[0] * ht.shape[2])\n",
    "            # compute a baseline value for the value network\n",
    "            bi = value_net(ht_)\n",
    "            # draw a stop decision\n",
    "            stop_decision, log_prob_s = sample_policy_s(ht, policy_s)\n",
    "            stop_decision = stop_decision.item()\n",
    "            if stop_decision == 1: # classify\n",
    "                break\n",
    "            else: \n",
    "                reread_or_skim_times += 1\n",
    "                # draw an action (reread or skip)\n",
    "                step, log_prob_n = sample_policy_n(ht, policy_n)\n",
    "                curr_step += int(step)  # reread or skip\n",
    "                if curr_step < CHUNCK_SIZE and count < 5:\n",
    "                    # If the code can still execute the next loop, it is not the last time step.\n",
    "                    cost_ep.append(clstm_cost + s_cost + n_cost)\n",
    "                    # add the baseline value\n",
    "                    baseline_value_ep.append(bi)\n",
    "                    # add the log prob for the current actions\n",
    "                    saved_log_probs.append(log_prob_s + log_prob_n)\n",
    "        # draw a predicted label\n",
    "        output_c = policy_c(ht)\n",
    "        # cross entrpy loss input shape: input(N, C), target(N)\n",
    "        loss = criterion(output_c, label)  # positive value\n",
    "        # draw a predicted label \n",
    "        pred_label, log_prob_c = sample_policy_c(output_c)\n",
    "        # update the confusion matrix\n",
    "        cm[pred_label][y] += 1\n",
    "        if stop_decision == 1:\n",
    "            # add the cost of the last time step\n",
    "            cost_ep.append(clstm_cost + s_cost + c_cost)\n",
    "            saved_log_probs.append(log_prob_s + log_prob_c)\n",
    "        else:\n",
    "            # At the moment, the probability of drawing a stop decision is 1,\n",
    "            # so its log probability is zero which can be ignored in th sum.\n",
    "            saved_log_probs.append(log_prob_c.unsqueeze(0))\n",
    "        # add the baseline value\n",
    "        baseline_value_ep.append(bi)\n",
    "        # add the cross entropy loss\n",
    "        encoder_loss_sum.append(loss)\n",
    "        # set reward for the current data sample\n",
    "        if pred_label.item() == label:\n",
    "            reward = 1 \n",
    "        else:\n",
    "            reward = -1 \n",
    "        # compute the policy losses and value losses for the current episode\n",
    "        policy_loss_ep = []\n",
    "        value_losses = []\n",
    "        for i, log_prob in enumerate(saved_log_probs):\n",
    "            # baseline_value_ep[i].item(): updating the policy loss doesn't include the gradient of baseline values\n",
    "            advantage = reward - baseline_value_ep[i].item()\n",
    "            policy_loss_ep.append(log_prob * advantage)\n",
    "            value_losses.append((reward - baseline_value_ep[i]) ** 2)\n",
    "        policy_loss_sum.append(torch.cat(policy_loss_ep).sum())\n",
    "        baseline_value_batch.append(torch.cat(value_losses).sum())\n",
    "        # update gradients\n",
    "        if (index + 1) % 32 == 0:  # take the average of samples, backprop\n",
    "            finish_episode(policy_loss_sum, encoder_loss_sum, baseline_value_batch)\n",
    "            del policy_loss_sum[:], encoder_loss_sum[:], baseline_value_batch[:]\n",
    "            \n",
    "        if (index + 1) % 32 == 0:\n",
    "            stats = calculate_stats_from_cm(cm)\n",
    "            cm = np.zeros((LABEL_DIM, LABEL_DIM))\n",
    "            acc = stats[\"accuracy\"]\n",
    "            recall = stats[\"recall\"]\n",
    "            precision = stats[\"precision\"]\n",
    "            f1 = stats[\"f1\"]\n",
    "            writer.add_scalar(\"train_accuracy\", acc, len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_recall\", recall,  len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_precision\", precision,  len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_f1\", f1,  len(train_loader)*epoch + index)\n",
    "            pbar.set_description(f\"episode: {index + 1}, reread_or_skim_times: {reread_or_skim_times}, accuracy: {acc:.3f}, precision: {precision:.3f}, recall: {recall:.3f}, f1: {f1:.2f}\")\n",
    "            \n",
    "            \"\"\"print(f'\\n current episode: {index + 1}')\n",
    "            # log the current position of the text which the agent has gone through\n",
    "            print('curr_step: ', curr_step)\n",
    "            # log the sum of the rereading and skimming times\n",
    "            print(f'current reread_or_skim_times: {reread_or_skim_times}')\"\"\"\n",
    "\n",
    "\n",
    "    print('Epoch time elapsed: %.2f s' % (time.time() - start))\n",
    "    print('reread_or_skim_times in this epoch:', reread_or_skim_times)\n",
    "    count_all, count_correct = evaluate_lm(trns_lstm, policy_s, policy_n, policy_c, valid_loader, 10)\n",
    "    print('Epoch: %s, Accuracy on the validation set: %.2f' % (epoch + 1, count_correct / count_all))\n",
    "    writer.add_scalar(\"validation_acccuracy\", count_correct / count_all,  len(train_loader)*epoch + index)\n",
    "    # count_all, count_correct = evaluate(clstm, policy_s, policy_n, policy_c, train_loader)\n",
    "    # print('Epoch: %s, Accuracy on the training set: %.2f' % (epoch + 1, count_correct / count_all))\n",
    "    \n",
    "print('Compute the accuracy on the testing set...')\n",
    "count_all, count_correct = evaluate_lm(trns_lstm, policy_s, policy_n, policy_c, test_loader, 10)\n",
    "print('Accuracy on the testing set: %.2f' % (count_correct / count_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_loss = optim.Adam(trns_lstm.parameters(), lr=1e-8)\n",
    "optim_policy = optim.Adam(params_pg, lr=1e-5)\n",
    "optim_value = optim.Adam(value_net.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training starts...')\n",
    "\n",
    "for epoch in range(5, 30):\n",
    "    print('\\nEpoch', epoch+1)\n",
    "    # log the start time of the epoch\n",
    "    start = time.time()\n",
    "    # set the models in training mode\n",
    "    trns_lstm.train()\n",
    "    policy_s.train()\n",
    "    policy_n.train()\n",
    "    policy_c.train()\n",
    "    # reset the count of reread_or_skim_times\n",
    "    reread_or_skim_times = 0\n",
    "    policy_loss_sum = []\n",
    "    encoder_loss_sum = []\n",
    "    baseline_value_batch = []\n",
    "    pbar = tqdm(train_loader)\n",
    "    cm = np.zeros((LABEL_DIM, LABEL_DIM))\n",
    "    for index, (x, xmask, y) in enumerate(pbar):\n",
    "        label = y.to(device)\n",
    "        text = x.to(device).view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE) # transform 1*400 to 20*1*20\n",
    "        text_mask = xmask.to(device).view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE)\n",
    "        curr_step = 0  # the position of the current chunk\n",
    "        h_0 = torch.zeros([NUM_RNN_LAYERS,1,HIDDEN_DIM_LSTM]).to(device)  # run on GPU\n",
    "        c_0 = torch.zeros([NUM_RNN_LAYERS,1,HIDDEN_DIM_LSTM]).to(device)\n",
    "        count = 0  # maximum skim/reread time: 5\n",
    "        baseline_value_ep = []\n",
    "        saved_log_probs = []  # for the use of policy gradient update\n",
    "        # collect the computational costs for every time step\n",
    "        cost_ep = []  \n",
    "        while curr_step < CHUNCK_SIZE and count < 5: \n",
    "            # Loop until a text can be classified or currstep is up to 20 or count reach the maximum i.e. 5.\n",
    "            # update count\n",
    "            count += 1\n",
    "            # pass the input through cnn-lstm and policy s\n",
    "            text_input = text[curr_step] # text_input 1*20\n",
    "            text_mask_input = text_mask[curr_step]\n",
    "            # print(f\"input h: {h_0.shape}\")\n",
    "            ht, ct = trns_lstm(text_input, text_mask_input, h_0, c_0)  #ht: NUM_RNN_LAYERS * 1 * HIDDEN_DIM_LSTM\n",
    "            # separate the value which is the input of value net\n",
    "            ht_ = ht.clone().detach().requires_grad_(True)\n",
    "            \n",
    "            # NUM_RNN_LAYERS * 1 * 128, next input of lstm\n",
    "            h_0 = ht # .unsqueeze(0)\n",
    "            c_0 = ct\n",
    "            \n",
    "            ht_ = ht_[-1, :, :]# .view(1, ht_.shape[0] * ht_.shape[2]) # ht_: 1, NUM_RNN_LAYERS * HIDDEN_DIM_LSTM\n",
    "            ht = ht[-1, :, :] # .view(1, ht.shape[0] * ht.shape[2])\n",
    "            # compute a baseline value for the value network\n",
    "            bi = value_net(ht_)\n",
    "            # draw a stop decision\n",
    "            stop_decision, log_prob_s = sample_policy_s(ht, policy_s)\n",
    "            stop_decision = stop_decision.item()\n",
    "            if stop_decision == 1: # classify\n",
    "                break\n",
    "            else: \n",
    "                reread_or_skim_times += 1\n",
    "                # draw an action (reread or skip)\n",
    "                step, log_prob_n = sample_policy_n(ht, policy_n)\n",
    "                curr_step += int(step)  # reread or skip\n",
    "                if curr_step < CHUNCK_SIZE and count < 10:\n",
    "                    # If the code can still execute the next loop, it is not the last time step.\n",
    "                    cost_ep.append(clstm_cost + s_cost + n_cost)\n",
    "                    # add the baseline value\n",
    "                    baseline_value_ep.append(bi)\n",
    "                    # add the log prob for the current actions\n",
    "                    saved_log_probs.append(log_prob_s + log_prob_n)\n",
    "        # draw a predicted label\n",
    "        output_c = policy_c(ht)\n",
    "        # cross entrpy loss input shape: input(N, C), target(N)\n",
    "        loss = criterion(output_c, label)  # positive value\n",
    "        # draw a predicted label \n",
    "        pred_label, log_prob_c = sample_policy_c(output_c)\n",
    "        # update the confusion matrix\n",
    "        cm[pred_label][y] += 1\n",
    "        if stop_decision == 1:\n",
    "            # add the cost of the last time step\n",
    "            cost_ep.append(clstm_cost + s_cost + c_cost)\n",
    "            saved_log_probs.append(log_prob_s + log_prob_c)\n",
    "        else:\n",
    "            # At the moment, the probability of drawing a stop decision is 1,\n",
    "            # so its log probability is zero which can be ignored in th sum.\n",
    "            saved_log_probs.append(log_prob_c.unsqueeze(0))\n",
    "        # add the baseline value\n",
    "        baseline_value_ep.append(bi)\n",
    "        # add the cross entropy loss\n",
    "        encoder_loss_sum.append(loss)\n",
    "        # set reward for the current data sample\n",
    "        if pred_label.item() == label:\n",
    "            reward = 1 \n",
    "        else:\n",
    "            reward = -1 \n",
    "        # compute the policy losses and value losses for the current episode\n",
    "        policy_loss_ep = []\n",
    "        value_losses = []\n",
    "        for i, log_prob in enumerate(saved_log_probs):\n",
    "            # baseline_value_ep[i].item(): updating the policy loss doesn't include the gradient of baseline values\n",
    "            advantage = reward - baseline_value_ep[i].item()\n",
    "            policy_loss_ep.append(log_prob * advantage)\n",
    "            value_losses.append((reward - baseline_value_ep[i]) ** 2)\n",
    "        policy_loss_sum.append(torch.cat(policy_loss_ep).sum())\n",
    "        baseline_value_batch.append(torch.cat(value_losses).sum())\n",
    "        # update gradients\n",
    "        if (index + 1) % 32 == 0:  # take the average of samples, backprop\n",
    "            finish_episode(policy_loss_sum, encoder_loss_sum, baseline_value_batch)\n",
    "            del policy_loss_sum[:], encoder_loss_sum[:], baseline_value_batch[:]\n",
    "            \n",
    "        if (index + 1) % 32 == 0:\n",
    "            stats = calculate_stats_from_cm(cm)\n",
    "            cm = np.zeros((LABEL_DIM, LABEL_DIM))\n",
    "            acc = stats[\"accuracy\"]\n",
    "            recall = stats[\"recall\"]\n",
    "            precision = stats[\"precision\"]\n",
    "            f1 = stats[\"f1\"]\n",
    "            writer.add_scalar(\"train_accuracy\", acc, len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_recall\", recall,  len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_precision\", precision,  len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_f1\", f1,  len(train_loader)*epoch + index)\n",
    "            pbar.set_description(f\"episode: {index + 1}, reread_or_skim_times: {reread_or_skim_times}, accuracy: {acc:.3f}, precision: {precision:.3f}, recall: {recall:.3f}, f1: {f1:.2f}\")\n",
    "            \n",
    "            \"\"\"print(f'\\n current episode: {index + 1}')\n",
    "            # log the current position of the text which the agent has gone through\n",
    "            print('curr_step: ', curr_step)\n",
    "            # log the sum of the rereading and skimming times\n",
    "            print(f'current reread_or_skim_times: {reread_or_skim_times}')\"\"\"\n",
    "\n",
    "\n",
    "    print('Epoch time elapsed: %.2f s' % (time.time() - start))\n",
    "    print('reread_or_skim_times in this epoch:', reread_or_skim_times)\n",
    "    count_all, count_correct = evaluate_lm(trns_lstm, policy_s, policy_n, policy_c, valid_loader,10)\n",
    "    print('Epoch: %s, Accuracy on the validation set: %.3f' % (epoch + 1, count_correct / count_all))\n",
    "    writer.add_scalar(\"validation_acccuracy\", count_correct / count_all,  len(train_loader)*epoch + index)\n",
    "    # count_all, count_correct = evaluate(clstm, policy_s, policy_n, policy_c, train_loader)\n",
    "    # print('Epoch: %s, Accuracy on the training set: %.2f' % (epoch + 1, count_correct / count_all))\n",
    "    \n",
    "print('Compute the accuracy on the testing set...')\n",
    "count_all, count_correct = evaluate_lm(trns_lstm, policy_s, policy_n, policy_c, test_loader,10)\n",
    "print('Accuracy on the testing set: %.2f' % (count_correct / count_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trns_lstm.distilbert = trns_lstm.distilbert.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_all, count_correct = evaluate(clstm, policy_s, policy_n, policy_c, valid_loader)\n",
    "# print('Epoch: %s, Accuracy on the validation set: %.2f' % (epoch + 1, count_correct / count_all))\n",
    "count_all, count_correct = evaluate_lm(trns_lstm, policy_s, policy_n, policy_c, train_loader)\n",
    "print('Epoch: %s, Accuracy on the training set: %.2f' % (epoch + 1, count_correct / count_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_models(time_str, date_str, path: str = \".\"):\n",
    "    os.makedirs(f\"{path}\\\\{date_str}\", exist_ok=True)\n",
    "    torch.save(trns_lstm, f\"{path}\\\\{date_str}\\\\clstm_{time_str}.pth\")\n",
    "    torch.save(policy_s, f\"{path}\\\\{date_str}\\\\policy_s_{time_str}.pth\")\n",
    "    torch.save(policy_n, f\"{path}\\\\{date_str}\\\\policy_n_{time_str}.pth\")\n",
    "    torch.save(policy_c, f\"{path}\\\\{date_str}\\\\policy_c_{time_str}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models(now_time, now_date, \"saved_models\\\\roberta_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trns_lstm.eval()\n",
    "policy_s.eval()\n",
    "policy_n.eval()\n",
    "policy_c.eval()\n",
    "action_logs = []\n",
    "seen_logs = []\n",
    "writer = SummaryWriter()\n",
    "for i, (x, xm, y) in enumerate(valid_loader):\n",
    "    print(i)\n",
    "    print(valid_data[\"text\"].iloc[i])\n",
    "    print(y)\n",
    "    action_log_batch = []\n",
    "    seen_batch = []\n",
    "    label = y.to(device).long() # for cross entropy loss, the long type is required\n",
    "    text = x.to(device).view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE) # transform 1*400 to 20*1*20\n",
    "    text_mask = xm.to(device).view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE)\n",
    "    curr_step = 0\n",
    "    h_0 = torch.zeros([NUM_RNN_LAYERS,1,HIDDEN_DIM_LSTM]).to(device)\n",
    "    c_0 = torch.zeros([NUM_RNN_LAYERS,1,HIDDEN_DIM_LSTM]).to(device)\n",
    "    count = 0\n",
    "    while curr_step < 20 and count < 5: # loop until a text can be classified or currstep is up to 20\n",
    "        count += 1\n",
    "        # pass the input through cnn-lstm and policy s\n",
    "        text_input = text[curr_step] # text_input 1*20\n",
    "        text_input_mask = text_mask[curr_step]\n",
    "        text_str = train_data[\"text\"].iloc[i]\n",
    "        seen_batch.append(text_str.split()[curr_step * 20: (curr_step+1)*20])\n",
    "        ht, ct = trns_lstm(text_input, text_input_mask, h_0, c_0)  # 1 * 128\n",
    "        # if count == 1 and i == 0:\n",
    "        #     writer.add_graph(clstm, [text_input, h_0, c_0], verbose=True)\n",
    "        h_0 = ht #.unsqueeze(0) # NUM_RNN_LAYERS * 1 * LSTM_HIDDEN_DIM, next input of lstm\n",
    "        c_0 = ct\n",
    "\n",
    "        ht = ht[-1, :, :] # .view(1, ht.shape[0] * ht.shape[2])\n",
    "        # ht_ = ht.view(1, ht.shape[0] * ht.shape[2])\n",
    "        # draw a stop decision\n",
    "        stop_decision, log_prob_s = sample_policy_s(ht, policy_s)\n",
    "        # if count == 1 and i == 1:\n",
    "        #     writer.add_graph(policy_s, ht)\n",
    "        stop_decision = stop_decision.item()\n",
    "        if stop_decision == 1: # classify\n",
    "            break\n",
    "        else:\n",
    "            # draw an action (reread or skip)\n",
    "            step, log_prob_n = sample_policy_n(ht, policy_n)\n",
    "            # if count == 1 and i == 2:\n",
    "            #     writer.add_graph(policy_n, ht)\n",
    "            curr_step += int(step)  # reread or skip\n",
    "            action_log_batch.append({\"skip/reread\": step})\n",
    "    # draw a predicted label\n",
    "    output_c = policy_c(ht)\n",
    "    if i == 3:\n",
    "        writer.add_graph(policy_c, ht, verbose=True)\n",
    "        \n",
    "    # draw a predicted label \n",
    "    pred_label, log_prob_c = sample_policy_c(output_c)\n",
    "    action_log_batch.append({\"prediction\": pred_label, \"real\": label})\n",
    "    if pred_label.item() == label:\n",
    "        count_correct += 1\n",
    "    count_all += 1\n",
    "    action_logs.append(action_log_batch)\n",
    "    seen_logs.append(seen_batch)\n",
    "    if i == 10:\n",
    "        break\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_logs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 3\n",
    "action_logs[ix], \" \".join(train_data[\"text\"].iloc[ix].split()[0:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(seen_logs[ix][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_correct / count_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
