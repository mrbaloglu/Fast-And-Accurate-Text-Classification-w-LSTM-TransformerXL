{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Bernoulli, Categorical\n",
    "from torchtext import datasets\n",
    "import os\n",
    "import time\n",
    "import numpy as np \n",
    "import random\n",
    "import argparse\n",
    "\n",
    "from networks import Distilbert_LSTM, Policy_C, Policy_N, Policy_S, ValueNetwork\n",
    "from utils.utils import sample_policy_c, sample_policy_n, sample_policy_s, evaluate_lm, compute_policy_value_losses\n",
    "from utils.utils import cnn_cost, clstm_cost, c_cost, n_cost, s_cost, openDfFromPickle, calculate_stats_from_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Number of training examples: 20000\n",
      "Number of validation examples: 5000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print('Reading data...')\n",
    "train_data = openDfFromPickle(\"C:\\\\Users\\\\mrbal\\\\Documents\\\\NLP\\\\RL\\\\basic_reinforcement_learning\\\\NLP_datasets\\\\imdb\\\\imdb_train_distilbert-base-uncased.pkl\")\n",
    "valid_data = openDfFromPickle(\"C:\\\\Users\\\\mrbal\\\\Documents\\\\NLP\\\\RL\\\\basic_reinforcement_learning\\\\NLP_datasets\\\\imdb\\\\imdb_val_distilbert-base-uncased.pkl\")\n",
    "test_data = openDfFromPickle(\"C:\\\\Users\\\\mrbal\\\\Documents\\\\NLP\\\\RL\\\\basic_reinforcement_learning\\\\NLP_datasets\\\\imdb\\\\imdb_test_distilbert-base-uncased.pkl\")\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_str</th>\n",
       "      <th>text_bert_input_ids</th>\n",
       "      <th>text_bert_attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23311</th>\n",
       "      <td>I borrowed this movie despite its extremely lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[101, 1045, 11780, 2023, 3185, 2750, 2049, 518...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23623</th>\n",
       "      <td>After the unexpected accident that killed an i...</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[101, 2044, 1996, 9223, 4926, 2008, 2730, 2019...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>On the summer blockbuster hit BASEketball This...</td>\n",
       "      <td>0</td>\n",
       "      <td>neg</td>\n",
       "      <td>[101, 2006, 1996, 2621, 27858, 2718, 2918, 348...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12645</th>\n",
       "      <td>Can Scarcely Imagine Better Movie Than This br...</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[101, 2064, 20071, 5674, 2488, 3185, 2084, 202...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>A still famous but decadent actor Morgan Freem...</td>\n",
       "      <td>0</td>\n",
       "      <td>neg</td>\n",
       "      <td>[101, 1037, 2145, 3297, 2021, 5476, 3372, 3364...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21575</th>\n",
       "      <td>My discovery of the cinema of Jan Svankmajer o...</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[101, 2026, 5456, 1997, 1996, 5988, 1997, 5553...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>The story is similar to ET an extraterrestrial...</td>\n",
       "      <td>0</td>\n",
       "      <td>neg</td>\n",
       "      <td>[101, 1996, 2466, 2003, 2714, 2000, 3802, 2019...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>I have read the novel Reaper of Ben Mezrich fe...</td>\n",
       "      <td>0</td>\n",
       "      <td>neg</td>\n",
       "      <td>[101, 1045, 2031, 3191, 1996, 3117, 19559, 199...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>Went to see this finnish film and ve got to sa...</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[101, 2253, 2000, 2156, 2023, 6983, 2143, 1998...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23654</th>\n",
       "      <td>I first saw Breaking Glass in and thought that...</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[101, 1045, 2034, 2387, 4911, 3221, 1999, 1998...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label label_str  \\\n",
       "23311  I borrowed this movie despite its extremely lo...      1       pos   \n",
       "23623  After the unexpected accident that killed an i...      1       pos   \n",
       "1020   On the summer blockbuster hit BASEketball This...      0       neg   \n",
       "12645  Can Scarcely Imagine Better Movie Than This br...      1       pos   \n",
       "1533   A still famous but decadent actor Morgan Freem...      0       neg   \n",
       "...                                                  ...    ...       ...   \n",
       "21575  My discovery of the cinema of Jan Svankmajer o...      1       pos   \n",
       "5390   The story is similar to ET an extraterrestrial...      0       neg   \n",
       "860    I have read the novel Reaper of Ben Mezrich fe...      0       neg   \n",
       "15795  Went to see this finnish film and ve got to sa...      1       pos   \n",
       "23654  I first saw Breaking Glass in and thought that...      1       pos   \n",
       "\n",
       "                                     text_bert_input_ids  \\\n",
       "23311  [101, 1045, 11780, 2023, 3185, 2750, 2049, 518...   \n",
       "23623  [101, 2044, 1996, 9223, 4926, 2008, 2730, 2019...   \n",
       "1020   [101, 2006, 1996, 2621, 27858, 2718, 2918, 348...   \n",
       "12645  [101, 2064, 20071, 5674, 2488, 3185, 2084, 202...   \n",
       "1533   [101, 1037, 2145, 3297, 2021, 5476, 3372, 3364...   \n",
       "...                                                  ...   \n",
       "21575  [101, 2026, 5456, 1997, 1996, 5988, 1997, 5553...   \n",
       "5390   [101, 1996, 2466, 2003, 2714, 2000, 3802, 2019...   \n",
       "860    [101, 1045, 2031, 3191, 1996, 3117, 19559, 199...   \n",
       "15795  [101, 2253, 2000, 2156, 2023, 6983, 2143, 1998...   \n",
       "23654  [101, 1045, 2034, 2387, 4911, 3221, 1999, 1998...   \n",
       "\n",
       "                                text_bert_attention_mask  \n",
       "23311  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "23623  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1020   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "12645  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1533   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "...                                                  ...  \n",
       "21575  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "5390   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "860    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "15795  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "23654  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "\n",
       "[20000 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After the unexpected accident that killed an inexperienced climber Michelle Joyner Eight months has passed The Rocky Mountain Rescue receive distress call set by brilliant terrorist mastermind Eric Quaien John Lithgow Quaien has lost three large cases that has millions of dollars inside Two experienced climbers Walker Sylvester Stallone and Tucker Micheal Rooker and helicopter pilot Janine Turner are to the rescue but they are set by trap by Quaien and his men Now the two climbers and pilot are forced to play deadly game of hide and seek While Quaien is trying to find the millions of dollars and he kidnapped Tucker to find the money Once Tucker finds the money Tucker will be dead Against explosive firepower bitter cold and dizzying heights Walker must outwit Quaien for survival br br Directed by Renny Harlin Driven Mindhunters Nightmare on Elm Street The Dream Master made an entertaining non stop action picture This film is spectacular exciting visually exciting action picture with plenty of dark humour as well This was one of the biggest hits of This is one of Harlin best film Lithgow is terrific entertaining villain Stallone certainly made an short comeback of this sharp thriller This is probably Harlin best work as filmmaker br br DVD has an sharp anamorphic Widescreen transfer and an terrific Dolby Digital Surround Sound DVD has an running commentary track by the director with comments by Stallone DVD also has technical crew commentary as well DVD has behind the scenes featurette two deleted scenes with introduction by the director and more Do not miss this great action film Screenplay by Micheal France Fantastic Four and actor Stallone The Rocky Series Based on premise by John Long Excellent Cinematography by Alex Thomson S Alien Demolition Man Legend Oscar Nominated for Best Sound Best Sound Editing and Best Visual Effects Panavision '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"text\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# split the datasets into batches\n",
    "BATCH_SIZE = 1  # the batch size for a dataset iterator\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20000, 400]) torch.Size([20000])\n",
      "torch.Size([5000, 400]) torch.Size([5000])\n",
      "torch.Size([25000, 400]) torch.Size([25000])\n"
     ]
    }
   ],
   "source": [
    "xtrain = torch.from_numpy(np.stack(train_data[\"text_bert_input_ids\"].values))[:, 0:400]\n",
    "xtrain_mask = torch.from_numpy(np.stack(train_data[\"text_bert_attention_mask\"].values))[:, 0:400]\n",
    "ytrain = torch.from_numpy(train_data[\"label\"].values)\n",
    "xvalid = torch.from_numpy(np.stack(valid_data[\"text_bert_input_ids\"].values))[:, 0:400]\n",
    "xvalid_mask = torch.from_numpy(np.stack(valid_data[\"text_bert_attention_mask\"].values))[:, 0:400]\n",
    "yvalid = torch.from_numpy(valid_data[\"label\"].values)\n",
    "xtest = torch.from_numpy(np.stack(test_data[\"text_bert_input_ids\"].values))[:, 0:400]\n",
    "xtest_mask = torch.from_numpy(np.stack(test_data[\"text_bert_attention_mask\"].values))[:, 0:400]\n",
    "ytest = torch.from_numpy(test_data[\"label\"].values)\n",
    "\n",
    "print(xtrain.shape, ytrain.shape)\n",
    "print(xvalid.shape, yvalid.shape)\n",
    "print(xtest.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "train_loader = DataLoader(TensorDataset(xtrain, xtrain_mask, ytrain), batch_size=BATCH_SIZE)\n",
    "valid_loader = DataLoader(TensorDataset(xvalid, xvalid_mask, yvalid), batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(TensorDataset(xtest, xtest_mask, ytest), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2023)\n",
    "np.random.seed(2023)\n",
    "torch.manual_seed(2023)\n",
    "torch.cuda.manual_seed(2023)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "NUM_RNN_LAYERS = 4\n",
    "HIDDEN_DIM_LSTM = 512 \n",
    "HIDDEN_DIM_DENSE = 1 * HIDDEN_DIM_LSTM # HIDDEN_DIM_LSTM * NUM_RNN_LAYERS\n",
    "OUTPUT_DIM = 1\n",
    "CHUNCK_SIZE = 20\n",
    "MAX_K = 4  # the output dimension for step size 0, 1, 2, 3\n",
    "LABEL_DIM = 2\n",
    "BATCH_SIZE = 1\n",
    "gamma = 0.99\n",
    "alpha = 0.2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb112d8c750e445fa96856661b1aecf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mrbal\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c47881b71664a4ab6e136418731d7af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at lvwerra/distilbert-imdb were not used when initializing DistilBertModel: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy start: torch.Size([1, 20])\n",
      "Transformer: torch.Size([1, 20, 768])\n",
      "permute: torch.Size([20, 1, 768])\n",
      "dummy start: torch.Size([1, 20])\n",
      "Transformer: torch.Size([1, 20, 768])\n",
      "permute: torch.Size([20, 1, 768])\n",
      "lstm out: torch.Size([20, 1, 512]), hidden: torch.Size([4, 1, 512]), cell: torch.Size([4, 1, 512])\n",
      "torch.Size([4, 1, 512])\n",
      "Distilbert_LSTM(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (lstm): LSTM(768, 512, num_layers=4)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# set up the criterion\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "# set up models\n",
    "trns_lstm = Distilbert_LSTM(NUM_RNN_LAYERS, HIDDEN_DIM_LSTM, bert_checkpoint=\"lvwerra/distilbert-imdb\").to(device)\n",
    "print(trns_lstm)\n",
    "policy_s = Policy_S(HIDDEN_DIM_DENSE, HIDDEN_DIM_DENSE, OUTPUT_DIM).to(device)\n",
    "policy_n = Policy_N(HIDDEN_DIM_DENSE, HIDDEN_DIM_DENSE, MAX_K).to(device)\n",
    "policy_c = Policy_C(HIDDEN_DIM_DENSE, HIDDEN_DIM_DENSE, LABEL_DIM).to(device)\n",
    "value_net = ValueNetwork(HIDDEN_DIM_DENSE, HIDDEN_DIM_DENSE, OUTPUT_DIM).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up optimiser\n",
    "params_pg = list(policy_s.parameters()) + list(policy_c.parameters()) + list(policy_n.parameters())\n",
    "optim_loss = optim.Adam(trns_lstm.parameters(), lr=learning_rate)\n",
    "optim_policy = optim.Adam(params_pg, lr=learning_rate)\n",
    "optim_value = optim.Adam(value_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode(policy_loss_sum, encoder_loss_sum, baseline_value_batch):\n",
    "    '''\n",
    "    Called when a data sample has been processed.\n",
    "    '''\n",
    "    baseline_value_sum = torch.stack(baseline_value_batch).sum()\n",
    "    policy_loss = torch.stack(policy_loss_sum).mean()\n",
    "    encoder_loss = torch.stack(encoder_loss_sum).mean()\n",
    "    objective_loss = encoder_loss - policy_loss + baseline_value_sum\n",
    "    # set gradient to zero\n",
    "    optim_loss.zero_grad()\n",
    "    optim_policy.zero_grad()\n",
    "    optim_value.zero_grad()\n",
    "    # back propagation\n",
    "    objective_loss.backward()\n",
    "    # gradient update\n",
    "    optim_loss.step()\n",
    "    optim_policy.step()\n",
    "    optim_value.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trns_lstm.distilbert.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts...\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 87513, accuracy: 0.719, precision: 0.742, recall: 0.727, f1: 0.72: 100%|██████████| 20000/20000 [32:23<00:00, 10.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 1943.52 s\n",
      "reread_or_skim_times in this epoch: 87513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [06:02<00:00, 13.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 362.44 s\n",
      "Average FLOPs per sample:  7072170\n",
      "Epoch: 1, Accuracy on the validation set: 0.79\n",
      "\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 99996, accuracy: 0.719, precision: 0.742, recall: 0.727, f1: 0.72: 100%|██████████| 20000/20000 [38:00<00:00,  8.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 2280.81 s\n",
      "reread_or_skim_times in this epoch: 99996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [05:47<00:00, 14.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 347.99 s\n",
      "Average FLOPs per sample:  7072170\n",
      "Epoch: 2, Accuracy on the validation set: 0.80\n",
      "\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 100000, accuracy: 0.875, precision: 0.878, recall: 0.878, f1: 0.87: 100%|██████████| 20000/20000 [36:45<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 2205.86 s\n",
      "reread_or_skim_times in this epoch: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [05:42<00:00, 14.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 342.15 s\n",
      "Average FLOPs per sample:  7072170\n",
      "Epoch: 3, Accuracy on the validation set: 0.79\n",
      "\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 99819, accuracy: 0.875, precision: 0.895, recall: 0.882, f1: 0.87: 100%|██████████| 20000/20000 [36:05<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 2165.53 s\n",
      "reread_or_skim_times in this epoch: 99819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [06:01<00:00, 13.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 361.29 s\n",
      "Average FLOPs per sample:  7072170\n",
      "Epoch: 4, Accuracy on the validation set: 0.81\n",
      "\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 100000, accuracy: 0.781, precision: 0.781, recall: 0.782, f1: 0.78: 100%|██████████| 20000/20000 [36:07<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 2167.28 s\n",
      "reread_or_skim_times in this epoch: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [05:40<00:00, 14.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 340.78 s\n",
      "Average FLOPs per sample:  7072170\n",
      "Epoch: 5, Accuracy on the validation set: 0.78\n",
      "Compute the accuracy on the testing set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 25000/25000 [28:46<00:00, 14.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 1726.08 s\n",
      "Average FLOPs per sample:  7072170\n",
      "Accuracy on the testing set: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training starts...')\n",
    "\n",
    "for epoch in range(5):\n",
    "    print('\\nEpoch', epoch+1)\n",
    "    # log the start time of the epoch\n",
    "    start = time.time()\n",
    "    # set the models in training mode\n",
    "    trns_lstm.train()\n",
    "    policy_s.train()\n",
    "    policy_n.train()\n",
    "    policy_c.train()\n",
    "    # reset the count of reread_or_skim_times\n",
    "    reread_or_skim_times = 0\n",
    "    policy_loss_sum = []\n",
    "    encoder_loss_sum = []\n",
    "    baseline_value_batch = []\n",
    "    pbar = tqdm(train_loader)\n",
    "    cm = np.zeros((LABEL_DIM, LABEL_DIM))\n",
    "    for index, (x, xmask, y) in enumerate(pbar):\n",
    "        label = y.to(device)\n",
    "        text = x.to(device).view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE) # transform 1*400 to 20*1*20\n",
    "        text_mask = xmask.to(device).view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE)\n",
    "        curr_step = 0  # the position of the current chunk\n",
    "        h_0 = torch.zeros([NUM_RNN_LAYERS,1,HIDDEN_DIM_LSTM]).to(device)  # run on GPU\n",
    "        c_0 = torch.zeros([NUM_RNN_LAYERS,1,HIDDEN_DIM_LSTM]).to(device)\n",
    "        count = 0  # maximum skim/reread time: 5\n",
    "        baseline_value_ep = []\n",
    "        saved_log_probs = []  # for the use of policy gradient update\n",
    "        # collect the computational costs for every time step\n",
    "        cost_ep = []  \n",
    "        while curr_step < CHUNCK_SIZE and count < 5: \n",
    "            # Loop until a text can be classified or currstep is up to 20 or count reach the maximum i.e. 5.\n",
    "            # update count\n",
    "            count += 1\n",
    "            # pass the input through cnn-lstm and policy s\n",
    "            text_input = text[curr_step] # text_input 1*20\n",
    "            text_mask_input = text_mask[curr_step]\n",
    "            # print(f\"input h: {h_0.shape}\")\n",
    "            ht, ct = trns_lstm(text_input, text_mask_input, h_0, c_0)  #ht: NUM_RNN_LAYERS * 1 * HIDDEN_DIM_LSTM\n",
    "            # separate the value which is the input of value net\n",
    "            ht_ = ht.clone().detach().requires_grad_(True)\n",
    "            \n",
    "            # NUM_RNN_LAYERS * 1 * 128, next input of lstm\n",
    "            h_0 = ht # .unsqueeze(0)\n",
    "            c_0 = ct\n",
    "            \n",
    "            ht_ = ht_[-1, :, :]# .view(1, ht_.shape[0] * ht_.shape[2]) # ht_: 1, NUM_RNN_LAYERS * HIDDEN_DIM_LSTM\n",
    "            ht = ht[-1, :, :] # .view(1, ht.shape[0] * ht.shape[2])\n",
    "            # compute a baseline value for the value network\n",
    "            bi = value_net(ht_)\n",
    "            # draw a stop decision\n",
    "            stop_decision, log_prob_s = sample_policy_s(ht, policy_s)\n",
    "            stop_decision = stop_decision.item()\n",
    "            if stop_decision == 1: # classify\n",
    "                break\n",
    "            else: \n",
    "                reread_or_skim_times += 1\n",
    "                # draw an action (reread or skip)\n",
    "                step, log_prob_n = sample_policy_n(ht, policy_n)\n",
    "                curr_step += int(step)  # reread or skip\n",
    "                if curr_step < CHUNCK_SIZE and count < 5:\n",
    "                    # If the code can still execute the next loop, it is not the last time step.\n",
    "                    cost_ep.append(clstm_cost + s_cost + n_cost)\n",
    "                    # add the baseline value\n",
    "                    baseline_value_ep.append(bi)\n",
    "                    # add the log prob for the current actions\n",
    "                    saved_log_probs.append(log_prob_s + log_prob_n)\n",
    "        # draw a predicted label\n",
    "        output_c = policy_c(ht)\n",
    "        # cross entrpy loss input shape: input(N, C), target(N)\n",
    "        loss = criterion(output_c, label)  # positive value\n",
    "        # draw a predicted label \n",
    "        pred_label, log_prob_c = sample_policy_c(output_c)\n",
    "        # update the confusion matrix\n",
    "        cm[pred_label][y] += 1\n",
    "        if stop_decision == 1:\n",
    "            # add the cost of the last time step\n",
    "            cost_ep.append(clstm_cost + s_cost + c_cost)\n",
    "            saved_log_probs.append(log_prob_s + log_prob_c)\n",
    "        else:\n",
    "            # At the moment, the probability of drawing a stop decision is 1,\n",
    "            # so its log probability is zero which can be ignored in th sum.\n",
    "            saved_log_probs.append(log_prob_c.unsqueeze(0))\n",
    "        # add the baseline value\n",
    "        baseline_value_ep.append(bi)\n",
    "        # add the cross entropy loss\n",
    "        encoder_loss_sum.append(loss)\n",
    "        # set reward for the current data sample\n",
    "        if pred_label.item() == label:\n",
    "            reward = 1 \n",
    "        else:\n",
    "            reward = -1 \n",
    "        # compute the policy losses and value losses for the current episode\n",
    "        policy_loss_ep = []\n",
    "        value_losses = []\n",
    "        for i, log_prob in enumerate(saved_log_probs):\n",
    "            # baseline_value_ep[i].item(): updating the policy loss doesn't include the gradient of baseline values\n",
    "            advantage = reward - baseline_value_ep[i].item()\n",
    "            policy_loss_ep.append(log_prob * advantage)\n",
    "            value_losses.append((reward - baseline_value_ep[i]) ** 2)\n",
    "        policy_loss_sum.append(torch.cat(policy_loss_ep).sum())\n",
    "        baseline_value_batch.append(torch.cat(value_losses).sum())\n",
    "        # update gradients\n",
    "        if (index + 1) % 32 == 0:  # take the average of samples, backprop\n",
    "            finish_episode(policy_loss_sum, encoder_loss_sum, baseline_value_batch)\n",
    "            del policy_loss_sum[:], encoder_loss_sum[:], baseline_value_batch[:]\n",
    "            \n",
    "        if (index + 1) % 32 == 0:\n",
    "            stats = calculate_stats_from_cm(cm)\n",
    "            cm = np.zeros((LABEL_DIM, LABEL_DIM))\n",
    "            acc = stats[\"accuracy\"]\n",
    "            recall = stats[\"recall\"]\n",
    "            precision = stats[\"precision\"]\n",
    "            f1 = stats[\"f1\"]\n",
    "            writer.add_scalar(\"train_accuracy\", acc, len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_recall\", recall,  len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_precision\", precision,  len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_f1\", f1,  len(train_loader)*epoch + index)\n",
    "            pbar.set_description(f\"episode: {index + 1}, reread_or_skim_times: {reread_or_skim_times}, accuracy: {acc:.3f}, precision: {precision:.3f}, recall: {recall:.3f}, f1: {f1:.2f}\")\n",
    "            \n",
    "            \"\"\"print(f'\\n current episode: {index + 1}')\n",
    "            # log the current position of the text which the agent has gone through\n",
    "            print('curr_step: ', curr_step)\n",
    "            # log the sum of the rereading and skimming times\n",
    "            print(f'current reread_or_skim_times: {reread_or_skim_times}')\"\"\"\n",
    "\n",
    "\n",
    "    print('Epoch time elapsed: %.2f s' % (time.time() - start))\n",
    "    print('reread_or_skim_times in this epoch:', reread_or_skim_times)\n",
    "    count_all, count_correct = evaluate_lm(trns_lstm, policy_s, policy_n, policy_c, valid_loader)\n",
    "    print('Epoch: %s, Accuracy on the validation set: %.2f' % (epoch + 1, count_correct / count_all))\n",
    "    writer.add_scalar(\"validation_acccuracy\", count_correct / count_all,  len(train_loader)*epoch + index)\n",
    "    # count_all, count_correct = evaluate(clstm, policy_s, policy_n, policy_c, train_loader)\n",
    "    # print('Epoch: %s, Accuracy on the training set: %.2f' % (epoch + 1, count_correct / count_all))\n",
    "    \n",
    "print('Compute the accuracy on the testing set...')\n",
    "count_all, count_correct = evaluate_lm(trns_lstm, policy_s, policy_n, policy_c, test_loader)\n",
    "print('Accuracy on the testing set: %.2f' % (count_correct / count_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts...\n",
      "\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 100000, accuracy: 0.812, precision: 0.816, recall: 0.816, f1: 0.81: 100%|██████████| 20000/20000 [36:19<00:00,  9.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 2179.48 s\n",
      "reread_or_skim_times in this epoch: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [05:54<00:00, 14.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 354.81 s\n",
      "Average FLOPs per sample:  7072170\n",
      "Epoch: 6, Accuracy on the validation set: 0.829\n",
      "\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 100000, accuracy: 0.781, precision: 0.841, recall: 0.794, f1: 0.78: 100%|██████████| 20000/20000 [36:27<00:00,  9.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 2187.63 s\n",
      "reread_or_skim_times in this epoch: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [05:57<00:00, 13.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 357.93 s\n",
      "Average FLOPs per sample:  7072170\n",
      "Epoch: 7, Accuracy on the validation set: 0.826\n",
      "\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4800, reread_or_skim_times: 24000, accuracy: 0.812, precision: 0.802, recall: 0.817, f1: 0.81:  24%|██▍       | 4800/20000 [08:43<27:38,  9.16it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m cm \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((LABEL_DIM, LABEL_DIM))\n\u001b[0;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m index, (x, xmask, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(pbar):\n\u001b[1;32m---> 20\u001b[0m     label \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     21\u001b[0m     text \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mview(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE) \u001b[39m# transform 1*400 to 20*1*20\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     text_mask \u001b[39m=\u001b[39m xmask\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mview(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Training starts...')\n",
    "\n",
    "for epoch in range(5, 30):\n",
    "    print('\\nEpoch', epoch+1)\n",
    "    # log the start time of the epoch\n",
    "    start = time.time()\n",
    "    # set the models in training mode\n",
    "    trns_lstm.train()\n",
    "    policy_s.train()\n",
    "    policy_n.train()\n",
    "    policy_c.train()\n",
    "    # reset the count of reread_or_skim_times\n",
    "    reread_or_skim_times = 0\n",
    "    policy_loss_sum = []\n",
    "    encoder_loss_sum = []\n",
    "    baseline_value_batch = []\n",
    "    pbar = tqdm(train_loader)\n",
    "    cm = np.zeros((LABEL_DIM, LABEL_DIM))\n",
    "    for index, (x, xmask, y) in enumerate(pbar):\n",
    "        label = y.to(device)\n",
    "        text = x.to(device).view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE) # transform 1*400 to 20*1*20\n",
    "        text_mask = xmask.to(device).view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE)\n",
    "        curr_step = 0  # the position of the current chunk\n",
    "        h_0 = torch.zeros([NUM_RNN_LAYERS,1,HIDDEN_DIM_LSTM]).to(device)  # run on GPU\n",
    "        c_0 = torch.zeros([NUM_RNN_LAYERS,1,HIDDEN_DIM_LSTM]).to(device)\n",
    "        count = 0  # maximum skim/reread time: 5\n",
    "        baseline_value_ep = []\n",
    "        saved_log_probs = []  # for the use of policy gradient update\n",
    "        # collect the computational costs for every time step\n",
    "        cost_ep = []  \n",
    "        while curr_step < CHUNCK_SIZE and count < 5: \n",
    "            # Loop until a text can be classified or currstep is up to 20 or count reach the maximum i.e. 5.\n",
    "            # update count\n",
    "            count += 1\n",
    "            # pass the input through cnn-lstm and policy s\n",
    "            text_input = text[curr_step] # text_input 1*20\n",
    "            text_mask_input = text_mask[curr_step]\n",
    "            # print(f\"input h: {h_0.shape}\")\n",
    "            ht, ct = trns_lstm(text_input, text_mask_input, h_0, c_0)  #ht: NUM_RNN_LAYERS * 1 * HIDDEN_DIM_LSTM\n",
    "            # separate the value which is the input of value net\n",
    "            ht_ = ht.clone().detach().requires_grad_(True)\n",
    "            \n",
    "            # NUM_RNN_LAYERS * 1 * 128, next input of lstm\n",
    "            h_0 = ht # .unsqueeze(0)\n",
    "            c_0 = ct\n",
    "            \n",
    "            ht_ = ht_[-1, :, :]# .view(1, ht_.shape[0] * ht_.shape[2]) # ht_: 1, NUM_RNN_LAYERS * HIDDEN_DIM_LSTM\n",
    "            ht = ht[-1, :, :] # .view(1, ht.shape[0] * ht.shape[2])\n",
    "            # compute a baseline value for the value network\n",
    "            bi = value_net(ht_)\n",
    "            # draw a stop decision\n",
    "            stop_decision, log_prob_s = sample_policy_s(ht, policy_s)\n",
    "            stop_decision = stop_decision.item()\n",
    "            if stop_decision == 1: # classify\n",
    "                break\n",
    "            else: \n",
    "                reread_or_skim_times += 1\n",
    "                # draw an action (reread or skip)\n",
    "                step, log_prob_n = sample_policy_n(ht, policy_n)\n",
    "                curr_step += int(step)  # reread or skip\n",
    "                if curr_step < CHUNCK_SIZE and count < 5:\n",
    "                    # If the code can still execute the next loop, it is not the last time step.\n",
    "                    cost_ep.append(clstm_cost + s_cost + n_cost)\n",
    "                    # add the baseline value\n",
    "                    baseline_value_ep.append(bi)\n",
    "                    # add the log prob for the current actions\n",
    "                    saved_log_probs.append(log_prob_s + log_prob_n)\n",
    "        # draw a predicted label\n",
    "        output_c = policy_c(ht)\n",
    "        # cross entrpy loss input shape: input(N, C), target(N)\n",
    "        loss = criterion(output_c, label)  # positive value\n",
    "        # draw a predicted label \n",
    "        pred_label, log_prob_c = sample_policy_c(output_c)\n",
    "        # update the confusion matrix\n",
    "        cm[pred_label][y] += 1\n",
    "        if stop_decision == 1:\n",
    "            # add the cost of the last time step\n",
    "            cost_ep.append(clstm_cost + s_cost + c_cost)\n",
    "            saved_log_probs.append(log_prob_s + log_prob_c)\n",
    "        else:\n",
    "            # At the moment, the probability of drawing a stop decision is 1,\n",
    "            # so its log probability is zero which can be ignored in th sum.\n",
    "            saved_log_probs.append(log_prob_c.unsqueeze(0))\n",
    "        # add the baseline value\n",
    "        baseline_value_ep.append(bi)\n",
    "        # add the cross entropy loss\n",
    "        encoder_loss_sum.append(loss)\n",
    "        # set reward for the current data sample\n",
    "        if pred_label.item() == label:\n",
    "            reward = 1 \n",
    "        else:\n",
    "            reward = -1 \n",
    "        # compute the policy losses and value losses for the current episode\n",
    "        policy_loss_ep = []\n",
    "        value_losses = []\n",
    "        for i, log_prob in enumerate(saved_log_probs):\n",
    "            # baseline_value_ep[i].item(): updating the policy loss doesn't include the gradient of baseline values\n",
    "            advantage = reward - baseline_value_ep[i].item()\n",
    "            policy_loss_ep.append(log_prob * advantage)\n",
    "            value_losses.append((reward - baseline_value_ep[i]) ** 2)\n",
    "        policy_loss_sum.append(torch.cat(policy_loss_ep).sum())\n",
    "        baseline_value_batch.append(torch.cat(value_losses).sum())\n",
    "        # update gradients\n",
    "        if (index + 1) % 32 == 0:  # take the average of samples, backprop\n",
    "            finish_episode(policy_loss_sum, encoder_loss_sum, baseline_value_batch)\n",
    "            del policy_loss_sum[:], encoder_loss_sum[:], baseline_value_batch[:]\n",
    "            \n",
    "        if (index + 1) % 32 == 0:\n",
    "            stats = calculate_stats_from_cm(cm)\n",
    "            cm = np.zeros((LABEL_DIM, LABEL_DIM))\n",
    "            acc = stats[\"accuracy\"]\n",
    "            recall = stats[\"recall\"]\n",
    "            precision = stats[\"precision\"]\n",
    "            f1 = stats[\"f1\"]\n",
    "            writer.add_scalar(\"train_accuracy\", acc, len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_recall\", recall,  len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_precision\", precision,  len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_f1\", f1,  len(train_loader)*epoch + index)\n",
    "            pbar.set_description(f\"episode: {index + 1}, reread_or_skim_times: {reread_or_skim_times}, accuracy: {acc:.3f}, precision: {precision:.3f}, recall: {recall:.3f}, f1: {f1:.2f}\")\n",
    "            \n",
    "            \"\"\"print(f'\\n current episode: {index + 1}')\n",
    "            # log the current position of the text which the agent has gone through\n",
    "            print('curr_step: ', curr_step)\n",
    "            # log the sum of the rereading and skimming times\n",
    "            print(f'current reread_or_skim_times: {reread_or_skim_times}')\"\"\"\n",
    "\n",
    "\n",
    "    print('Epoch time elapsed: %.2f s' % (time.time() - start))\n",
    "    print('reread_or_skim_times in this epoch:', reread_or_skim_times)\n",
    "    count_all, count_correct = evaluate_lm(trns_lstm, policy_s, policy_n, policy_c, valid_loader)\n",
    "    print('Epoch: %s, Accuracy on the validation set: %.3f' % (epoch + 1, count_correct / count_all))\n",
    "    writer.add_scalar(\"validation_acccuracy\", count_correct / count_all,  len(train_loader)*epoch + index)\n",
    "    # count_all, count_correct = evaluate(clstm, policy_s, policy_n, policy_c, train_loader)\n",
    "    # print('Epoch: %s, Accuracy on the training set: %.2f' % (epoch + 1, count_correct / count_all))\n",
    "    \n",
    "print('Compute the accuracy on the testing set...')\n",
    "count_all, count_correct = evaluate_lm(trns_lstm, policy_s, policy_n, policy_c, test_loader)\n",
    "print('Accuracy on the testing set: %.2f' % (count_correct / count_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trns_lstm.distilbert = trns_lstm.distilbert.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 20000/20000 [05:23<00:00, 61.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 323.01 s\n",
      "Average FLOPs per sample:  2639562\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# count_all, count_correct = evaluate(clstm, policy_s, policy_n, policy_c, valid_loader)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# print('Epoch: %s, Accuracy on the validation set: %.2f' % (epoch + 1, count_correct / count_all))\u001b[39;00m\n\u001b[0;32m      3\u001b[0m count_all, count_correct \u001b[39m=\u001b[39m evaluate_lm(trns_lstm, policy_s, policy_n, policy_c, train_loader)\n\u001b[1;32m----> 4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, Accuracy on the training set: \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, count_correct \u001b[39m/\u001b[39m count_all))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    }
   ],
   "source": [
    "# count_all, count_correct = evaluate(clstm, policy_s, policy_n, policy_c, valid_loader)\n",
    "# print('Epoch: %s, Accuracy on the validation set: %.2f' % (epoch + 1, count_correct / count_all))\n",
    "count_all, count_correct = evaluate_lm(trns_lstm, policy_s, policy_n, policy_c, train_loader)\n",
    "print('Epoch: %s, Accuracy on the training set: %.2f' % (epoch + 1, count_correct / count_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('31_5_2023', '31_5_2023_10_1')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "now_time = str(now.day) + \"_\" + str(now.month) + \"_\" + str(now.year) + \"_\" + str(now.hour) + \"_\" + str(now.minute)\n",
    "now_date = str(now.day) + \"_\" + str(now.month) + \"_\" + str(now.year)\n",
    "now_date, now_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_models(time_str, date_str, path: str = \".\"):\n",
    "    os.makedirs(f\"{path}\\\\{date_str}\", exist_ok=True)\n",
    "    torch.save(trns_lstm, f\"{path}\\\\{date_str}\\\\clstm_{time_str}.pth\")\n",
    "    torch.save(policy_s, f\"{path}\\\\{date_str}\\\\policy_s_{time_str}.pth\")\n",
    "    torch.save(policy_n, f\"{path}\\\\{date_str}\\\\policy_n_{time_str}.pth\")\n",
    "    torch.save(policy_c, f\"{path}\\\\{date_str}\\\\policy_c_{time_str}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models(now_time, now_date, \"saved_models\\\\distilbert_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Dumb is as dumb does in this thoroughly uninteresting supposed black comedy Essentially what starts out as Chris Klein trying to maintain low profile eventually morphs into an uninspired version of The Three Amigos only without any laughs In order for black comedy to work it must be outrageous which Play Dead is not In order for black comedy to work it cannot be mean spirited which Play Dead is What Play Dead really is is town full of nut jobs Fred Dunst does however do pretty fair imitation of Billy Bob Thornton character from Simple Plan while Jake Busey does pretty fair imitation of well Jake Busey MERK\n",
      "tensor([0])\n",
      "1\n",
      "I dug out from my garage some old musicals and this is another one of my favorites It was written by Jay Alan Lerner and directed by Vincent Minelli It won two Academy Awards for Best Picture of and Best Screenplay The story of an American painter in Paris who tries to make it big Nina Foch is sophisticated lady of means and is very interested in helping him but soon finds she loves the guy Meanwhile Gene Kelly falls for lovely damsel Leslie Caron His main dancing partner and must say they are fantastic together on the floor and otherwise Famous French singer Georges Guietary sings too So if you like good smooth dancing and fun filled scenes filled with Oscar Levant nimble piano fingers the songs of George Gershwyn will live on forever in this colorful gem \n",
      "tensor([1])\n",
      "2\n",
      "After watching this movie was honestly disappointed not because of the actors story or directing was disappointed by this film advertisements br br The trailers were suggesting that the battalion have chosen the third way out other than surrender or die Polish infos were even misguiding that they had the choice between being killed by own artillery or German guns they even translated the title wrong as misplaced battalion This have tickled the right spot and bought the movie br br The disappointment started when realized that the third way is to just sit down and count dead bodies followed by sitting down and counting dead bodies Then began to think hey this story can be that simple bet this clever officer will find some cunning way to save what left of his troops Well he didn they were just sitting and waiting for something to happen And so was br br The story was based on real events of World War so the writers couldn make much use of their imagination but even thought found this movie really unchallenging and even little bit boring And as wrote in the first place it isn fault of actors writers or director their marketing people have raised my expectations high above the level that this movie could cope with \n",
      "tensor([0])\n",
      "3\n",
      "This movie was nominated for best picture but lost out to Casablanca but Paul Lukas beat out Humphrey Bogart for best actor don see why Lucile Watson was nominated for best supporting actor just don think she did very good job Bette Davis and Paul Lukas and their three kids are leaving Mexico and coming into the United States in the first scene of the movie They are going by train to Davis relatives house Davis and Lukas were in the underground to stop the Nazis so they are very tired and need rest But when they arrive home their is Nazi living there and their not much either can do about it It turns out the Nazi only cares about money and is willing to make deal with Lukas Their is more to the plot but you can find that out for yourself \n",
      "tensor([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\torch\\jit\\_trace.py:154: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:491.)\n",
      "  if a.grad is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self.1 : __torch__.networks.Policy_C,\n",
      "      %ht : Float(1, 512, strides=[512, 1], requires_grad=1, device=cuda:0)):\n",
      "  %fc2 : __torch__.torch.nn.modules.linear.___torch_mangle_0.Linear = prim::GetAttr[name=\"fc2\"](%self.1)\n",
      "  %relu : __torch__.torch.nn.modules.activation.ReLU = prim::GetAttr[name=\"relu\"](%self.1)\n",
      "  %dropout : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout\"](%self.1)\n",
      "  %fc1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"fc1\"](%self.1)\n",
      "  %bias.1 : Tensor = prim::GetAttr[name=\"bias\"](%fc1)\n",
      "  %weight.1 : Tensor = prim::GetAttr[name=\"weight\"](%fc1)\n",
      "  %input.1 : Float(1, 512, strides=[512, 1], requires_grad=1, device=cuda:0) = aten::linear(%ht, %weight.1, %bias.1), scope: __module.fc1 # c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114:0\n",
      "  %49 : bool = prim::Constant[value=0](), scope: __module.dropout # c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\torch\\nn\\functional.py:1252:0\n",
      "  %50 : float = prim::Constant[value=0.5](), scope: __module.dropout # c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\torch\\nn\\functional.py:1252:0\n",
      "  %input.3 : Float(1, 512, strides=[512, 1], requires_grad=1, device=cuda:0) = aten::dropout(%input.1, %50, %49), scope: __module.dropout # c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\torch\\nn\\functional.py:1252:0\n",
      "  %input : Float(1, 512, strides=[512, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.3), scope: __module.relu # c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\torch\\nn\\functional.py:1457:0\n",
      "  %bias : Tensor = prim::GetAttr[name=\"bias\"](%fc2)\n",
      "  %weight : Tensor = prim::GetAttr[name=\"weight\"](%fc2)\n",
      "  %55 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = aten::linear(%input, %weight, %bias), scope: __module.fc2 # c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114:0\n",
      "  return (%55)\n",
      "\n",
      "4\n",
      "Just like Al Gore shook us up with his painfully honest and cleverly presented documentary movie An inconvenient truth directors Alastair Fothergill and Mark Linfield also remind us that it about time to improve our way of life in order to save our beautiful planet Planet earth is also wake up call that the global warming of our planet has disastrous consequences for all living creatures around the world Al Gore showed us the bleak future of planet Earth by presenting hard facts backed up by documented examples through long yet always interesting monologues The creators of this documentary choose different yet equally powerful way to accomplish this They do not present us with future representation of what might occur to our planet if we don radically change things around but they rather show us the genuine beauty of planet Earth in all of its amazing glory We see places that we knew that existed but never thought they could be so beautiful In this movie we see wide array of the most extraordinary places such as forsaken deserts giant forests full of fauna and flora and icy landscapes as far as the eye could see And in all of those immensely different environments we see the most beautiful animals trying to survive br br This is exactly the kind of movie that had to be made in combination with the one from Al Gore in order to make us realize that our planet is too precious to meddle with The voice over by Patrick Stewart is always relaxing and thus very well done although at first it sounded as though was watching an men movie instead The cinematography is probably the most remarkable thing of this documentary At times what you see is so unreal that you tend to forget that man with camera actually had to film all of that delightful footage br br In short This is definitely must see for everyone since it concerns every single person on this beautiful planet Earth The truth is never thought our planet was so astonishingly beautiful \n",
      "tensor([1])\n",
      "5\n",
      "Unfortunately this movie is so bad The original Out of Towners was manic and very funny of course they used the script written by Neil Simon For some reason Neil Simons script is not used in this film so it falls flat time and time again Even the audience was with never laughed The direction is very slow and tedious and when there is joke it is given away so the joke dies e The couple having sex in the park They announce it is lighting ceremony for New York well we all know the lights are going to come on and we will be able to see cute and mugging Goldie Steve do bit of slap stick The whole movie winds up being like this joke is set up and given away Why isn Goldies hair ever even messed up in the movie You will also notice every close up of Goldie they use very intense soft lens suggest you rent the original with Jack Lemmon and Sandy Dennis that if you want to laugh \n",
      "tensor([0])\n",
      "6\n",
      "Altered Species starts one Friday night in Los Angeles where Dr Irwin Guy Vieg his laboratory assistant Walter Allen Lee Haff are burning the midnight oil as they continue to try perfect revolutionary new drug called Rejenacyn As Walter tips the latest failed attempt down the sink the pipes leak the florescent green liquid into the basement where escaped lab rats begin to drink it Five of Walter friends Alicia Leah Rown in very fetching outfit including some cool boots that she gets to stomp on rat with Gary Richard Peterson Burke Derek Hofman Frank David Bradley Chelsea Alexandra Townsend decide that he has been working too hard needs to get out so they plan to pick him up party the night away Back at the lab the cleaner Douglas Robert Broughton has been attacked killed by the now homicidal rats in the basement as Walter injects the latest batch of serum in lab rat which breaks out of it cage as it grows at an amazing rate Walter friends turn up but he can leave while the rat is still missing so everyone helps him look for it All six become potential rat food br br Also known as Rodentz Altered Species was co edited directed by Miles Feldman has very little to recommend it The script by producer Serge Rodnunsky is poor coupled together with the general shoddiness of the production as whole Altered Species really is lame For start the character are dumb annoying clich Then there the unoriginal plot with the mad scientist the monster he has created the isolated location the stranded human cast the obligatory final showdown between hero monster It all here somewhere Altered Species moves along at fair pace which is just about the best thing can say about it thankfully doesn last that long It basically your average run of the mill killer mutant rat film not particularly good one at that either br br Director Feldman films like TV film the whole thing is throughly bland forgettable while some of the special effects attack scenes leave lot to be desired For start the CGI rats are awful the attack sequences feature hand held jerky camera movement really quick edits to try hide the fact that all the rats are just passively sitting there At various points in Altered Species the rat cages need to shake because of the rats movement but you can clearly see all the rats just sitting there as someone shakes the cages off screen The giant rat monster at the end looks pretty poor as it just guy in dodgy suit There are no scares no tension or atmosphere since when did basements contain bright neon lighting There are one or two nice bits of gore here someone has nice big messy hole where their face used to be there a severed arm decapitation lots of rat bites someone having their eyeball yanked out dead mutilated cat br br Technically Altered Species is sub standard throughout It takes place within the confines of one building has cheap looking CGI effects low production values The acting isn up to much but it isn too bad special mention to Leah Rowan as Alicia as she a bit of babe makes Altered Species just that little bit nicer easier to watch br br Altered Species isn a particularly good film in fact it a pretty bad one but suppose you could do worse Not great but it might be worth watch if your not too demanding have nothing else to do \n",
      "tensor([0])\n",
      "7\n",
      " The Luzhin Defence is movie worthy of anyone time it is brooding intense film and kept my attention the entire time John Turturro is absolutely stunning in his portrayal of tender eccentric chess Grandmaster and Emily Watson is spell binding as the gentle but rebellious daughter of highly respected Russian family The chemistry between Watson and Turturro on screen is obvious from the moment their characters meet in the story All in all this movie is one of the best in depth looks at the life of chess Grandmaster and Turturro and Watson add whole non mainstream non cliche feel to the film Most people will come out of the theater thinking and feeling somewhat touched by this brilliant look at the most unlikely of love stories \n",
      "tensor([1])\n",
      "8\n",
      "This film trailer interested me enough to warrant renting the DVD However the resulting movie is absolutely dire Admittedly this is not the worst film ever made or the worst film this year but it came damn close br br The main issue is the film not knowing what it wants to be comedy adult drama thriller teen porn The story is interesting as it deals with the pitfalls of mail order brides but the film is mess What starts out as mildly interesting comedy word use in the loosest possible terms then goes totally in reverse and degenerates into very dark and distasteful misogynistic thriller Nicole Kidman should know better and Ben Chaplin is wasted As are Matthieu Kassovitz and Vincent Cassel whom can only presume did this for the money br br This is bad film in pretty much every single aspect It not funny it almost so sexist that you could almost forgive Benny Hill for everything he did and the dramatic elements are just downright nasty film to be avoided unless you absolutely have to see Kidman or Chaplin in every one of their films \n",
      "tensor([0])\n",
      "9\n",
      "The only reason watched this film was because had recently read Robert Hough less than perfect but interesting fictionalised account of the life of Big Cat trainer Mabel Stark Beaty appears as character in the book in less than flattering light br br hadn realised until checking the movie out later on the IMDb that it was originally serial Whoever edited the original running time of minutes down to the minuted version available on DVD has done hell of good job The shortened version plays just as well as any movie of the period despite the many duh what moments For instance are we really expected to believe our hero dug that twenty foot deep tiger trap in morning without even getting his jodhpurs dirty Looking over the chapter titles see that number five is titled Gorilla Warfare and number eleven is called The Gorilla There were no gorillas at all in the movie guess that where some of the cuts were made br br Historicaly interesting \n",
      "tensor([0])\n",
      "10\n",
      "I have to say that Higher Learning is one of the top movies have ever watched It has brilliant cast and an equally brilliant director Singleton shows how life in University can be There are main story lines the skinheads the African Americans and the homosexuals was intrigued by all of the stories but the one that got to me the most was the storyline about Kristen battling her feelings towards another girl The end was great After seeing the movie times plus still cry would have given this movie an but have to settlefor \n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "trns_lstm.eval()\n",
    "policy_s.eval()\n",
    "policy_n.eval()\n",
    "policy_c.eval()\n",
    "action_logs = []\n",
    "seen_logs = []\n",
    "writer = SummaryWriter()\n",
    "for i, (x, xm, y) in enumerate(valid_loader):\n",
    "    print(i)\n",
    "    print(valid_data[\"text\"].iloc[i])\n",
    "    print(y)\n",
    "    action_log_batch = []\n",
    "    seen_batch = []\n",
    "    label = y.to(device).long() # for cross entropy loss, the long type is required\n",
    "    text = x.to(device).view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE) # transform 1*400 to 20*1*20\n",
    "    text_mask = xm.to(device).view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE)\n",
    "    curr_step = 0\n",
    "    h_0 = torch.zeros([NUM_RNN_LAYERS,1,HIDDEN_DIM_LSTM]).to(device)\n",
    "    c_0 = torch.zeros([NUM_RNN_LAYERS,1,HIDDEN_DIM_LSTM]).to(device)\n",
    "    count = 0\n",
    "    while curr_step < 20 and count < 5: # loop until a text can be classified or currstep is up to 20\n",
    "        count += 1\n",
    "        # pass the input through cnn-lstm and policy s\n",
    "        text_input = text[curr_step] # text_input 1*20\n",
    "        text_input_mask = text_mask[curr_step]\n",
    "        text_str = train_data[\"text\"].iloc[i]\n",
    "        seen_batch.append(text_str.split()[curr_step * 20: (curr_step+1)*20])\n",
    "        ht, ct = trns_lstm(text_input, text_input_mask, h_0, c_0)  # 1 * 128\n",
    "        # if count == 1 and i == 0:\n",
    "        #     writer.add_graph(clstm, [text_input, h_0, c_0], verbose=True)\n",
    "        h_0 = ht #.unsqueeze(0) # NUM_RNN_LAYERS * 1 * LSTM_HIDDEN_DIM, next input of lstm\n",
    "        c_0 = ct\n",
    "\n",
    "        ht = ht[-1, :, :] # .view(1, ht.shape[0] * ht.shape[2])\n",
    "        # ht_ = ht.view(1, ht.shape[0] * ht.shape[2])\n",
    "        # draw a stop decision\n",
    "        stop_decision, log_prob_s = sample_policy_s(ht, policy_s)\n",
    "        # if count == 1 and i == 1:\n",
    "        #     writer.add_graph(policy_s, ht)\n",
    "        stop_decision = stop_decision.item()\n",
    "        if stop_decision == 1: # classify\n",
    "            break\n",
    "        else:\n",
    "            # draw an action (reread or skip)\n",
    "            step, log_prob_n = sample_policy_n(ht, policy_n)\n",
    "            # if count == 1 and i == 2:\n",
    "            #     writer.add_graph(policy_n, ht)\n",
    "            curr_step += int(step)  # reread or skip\n",
    "            action_log_batch.append({\"skip/reread\": step})\n",
    "    # draw a predicted label\n",
    "    output_c = policy_c(ht)\n",
    "    if i == 3:\n",
    "        writer.add_graph(policy_c, ht, verbose=True)\n",
    "        \n",
    "    # draw a predicted label \n",
    "    pred_label, log_prob_c = sample_policy_c(output_c)\n",
    "    action_log_batch.append({\"prediction\": pred_label, \"real\": label})\n",
    "    if pred_label.item() == label:\n",
    "        count_correct += 1\n",
    "    count_all += 1\n",
    "    action_logs.append(action_log_batch)\n",
    "    seen_logs.append(seen_batch)\n",
    "    if i == 10:\n",
    "        break\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'skip/reread': 1},\n",
       " {'skip/reread': 1},\n",
       " {'skip/reread': 1},\n",
       " {'skip/reread': 1},\n",
       " {'skip/reread': 1},\n",
       " {'prediction': tensor([0], device='cuda:0'),\n",
       "  'real': tensor([0], device='cuda:0')}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_logs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'skip/reread': 2},\n",
       "  {'skip/reread': 2},\n",
       "  {'skip/reread': 2},\n",
       "  {'skip/reread': 2},\n",
       "  {'skip/reread': 2},\n",
       "  {'prediction': tensor([1], device='cuda:0'),\n",
       "   'real': tensor([1], device='cuda:0')}],\n",
       " 'Can Scarcely Imagine Better Movie Than This br br Hey before you all go Chick Flick on me am very Large Strong Masculine Macho Man who happens to think this was one of the better movies of the last years br br The acting was Superb and the Story was Marvelous This is wonderful medicine for the heart and soul The Acting could not have been better nor the movie better cast br br have known for Good while that Mercedes Ruehl along with Holly Hunter Joan Plowright Dame Edith Evans Sissy Spacek Judi Dench is among the greatest actresses ever to appear on film And of course Cloris Leachman also in this film in my view may in fact exceed them all in the shear magnum of her talent and varied roles she has appeared in over the years At any rate this was an Amazing cast This film was like book that you cannot lay down and when you have reached the last page wish for more still more cannot for the life of me understand why this film here on the IMDb only rates br br That rating here is utterly Amazing to me Or perhaps not Perhaps in fact do understand it ever so well and that is what makes me really sad It makes me ever so sad that films like American Beauty Leaving Las Vegas Sexy Beast and Fight Club ratings skyrocket off the charts in popularity when they in fact at least in this viewers opinion should have received an rating that is for Rubbish Hey k realize there are lot of different stories in this world for lot of different audiences but it is sad commentary when this lovely powerful extraordinarily Directed Acted and written film seems to be over looked br br It obviously was at the Academy Awards as well How Sad And How predictable My summation is that if you want to see powerful Happy Sad beautiful story watch preferably own this film')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix = 3\n",
    "action_logs[ix], \" \".join(train_data[\"text\"].iloc[ix].split()[0:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(seen_logs[ix][\u001b[39m5\u001b[39;49m])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\" \".join(seen_logs[ix][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                        Dumb is as dumb does in this thoroughly uninte...\n",
       "label                                                                       0\n",
       "label_str                                                                 neg\n",
       "text_bert_input_ids         [101, 12873, 2003, 2004, 12873, 2515, 1999, 20...\n",
       "text_bert_attention_mask    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "Name: 6868, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9831278390655419"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_correct / count_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
