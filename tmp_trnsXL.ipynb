{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Bernoulli, Categorical\n",
    "from torchtext import datasets\n",
    "import os\n",
    "import time\n",
    "import numpy as np \n",
    "import random\n",
    "import argparse\n",
    "\n",
    "from buffer import Buffer\n",
    "from networks import Policy_C, Policy_N, Policy_S, ValueNetwork, Transformer\n",
    "from utils.utils import sample_policy_c, sample_policy_n, sample_policy_s, evaluate_transformer, compute_policy_value_losses\n",
    "from utils.utils import cnn_cost, clstm_cost, c_cost, n_cost, s_cost, openDfFromPickle, calculate_stats_from_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Number of training examples: 20000\n",
      "Number of validation examples: 5000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print('Reading data...')\n",
    "train_data = openDfFromPickle(\"C:\\\\Users\\\\mrbal\\\\Documents\\\\NLP\\\\RL\\\\basic_reinforcement_learning\\\\NLP_datasets\\\\imdb\\\\imdb_train_distilbert-base-uncased.pkl\")\n",
    "valid_data = openDfFromPickle(\"C:\\\\Users\\\\mrbal\\\\Documents\\\\NLP\\\\RL\\\\basic_reinforcement_learning\\\\NLP_datasets\\\\imdb\\\\imdb_val_distilbert-base-uncased.pkl\")\n",
    "test_data = openDfFromPickle(\"C:\\\\Users\\\\mrbal\\\\Documents\\\\NLP\\\\RL\\\\basic_reinforcement_learning\\\\NLP_datasets\\\\imdb\\\\imdb_test_distilbert-base-uncased.pkl\")\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "# split the datasets into batches\n",
    "BATCH_SIZE = 1  # the batch size for a dataset iterator\n",
    "device = torch.device(\"cpu\") # torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20000, 400]) torch.Size([20000])\n",
      "torch.Size([5000, 400]) torch.Size([5000])\n",
      "torch.Size([25000, 400]) torch.Size([25000])\n"
     ]
    }
   ],
   "source": [
    "xtrain = torch.from_numpy(np.stack(train_data[\"text_bert_input_ids\"].values))[:, 0:400]\n",
    "ytrain = torch.from_numpy(train_data[\"label\"].values)\n",
    "xvalid = torch.from_numpy(np.stack(valid_data[\"text_bert_input_ids\"].values))[:, 0:400]\n",
    "yvalid = torch.from_numpy(valid_data[\"label\"].values)\n",
    "xtest = torch.from_numpy(np.stack(test_data[\"text_bert_input_ids\"].values))[:, 0:400]\n",
    "ytest = torch.from_numpy(test_data[\"label\"].values)\n",
    "\n",
    "print(xtrain.shape, ytrain.shape)\n",
    "print(xvalid.shape, yvalid.shape)\n",
    "print(xtest.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "train_loader = DataLoader(TensorDataset(xtrain, ytrain), batch_size=BATCH_SIZE)\n",
    "valid_loader = DataLoader(TensorDataset(xvalid, yvalid), batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(TensorDataset(xtest, ytest), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2023)\n",
    "np.random.seed(2023)\n",
    "torch.manual_seed(2023)\n",
    "torch.cuda.manual_seed(2023)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "INPUT_DIM = 30522\n",
    "CHUNCK_SIZE = 20\n",
    "EMBEDDING_DIM = 100\n",
    "NUM_RNN_LAYERS = 1\n",
    "KER_SIZE = 5\n",
    "HIDDEN_DIM_LSTM = 128 \n",
    "HIDDEN_DIM_DENSE = CHUNCK_SIZE * 10 # HIDDEN_DIM_LSTM * NUM_RNN_LAYERS\n",
    "OUTPUT_DIM = 1\n",
    "MAX_K = 4  # the output dimension for step size 0, 1, 2, 3\n",
    "LABEL_DIM = 2\n",
    "N_FILTERS = 128\n",
    "BATCH_SIZE = 1\n",
    "gamma = 0.99\n",
    "alpha = 0.2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (activation): ReLU()\n",
      "  (linear_embedding): Embedding(30528, 100)\n",
      "  (conv): Conv2d(100, 10, kernel_size=(4, 4), stride=(1, 1), padding=same)\n",
      "  (transformer_blocks): ModuleList(\n",
      "    (0-2): 3 x TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (values): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (keys): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (queries): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (fc_out): Linear(in_features=200, out_features=200, bias=True)\n",
      "      )\n",
      "      (gate1): GRUGate(\n",
      "        (Wr): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Ur): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Wz): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Uz): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Wg): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Ug): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (sigmoid): Sigmoid()\n",
      "        (tanh): Tanh()\n",
      "      )\n",
      "      (gate2): GRUGate(\n",
      "        (Wr): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Ur): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Wz): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Uz): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Wg): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Ug): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (sigmoid): Sigmoid()\n",
      "        (tanh): Tanh()\n",
      "      )\n",
      "      (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_kv): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# set up the criterion\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "# set up models\n",
    "transformer_config = {\n",
    "    \"num_blocks\": 3,\n",
    "    \"embed_dim\": 100, \n",
    "    \"trns_input_dim\": 20*10, # embedding dim (per word) * chunk_size\n",
    "    \"num_heads\": 1,\n",
    "    \"memory_length\": 20,\n",
    "    \"positional_encoding\": \"\", # options: \"\" \"relative\" \"learned\"\n",
    "    \"layer_norm\": \"pre\", # options: \"\" \"pre\" \"post\"\n",
    "    \"gtrxl\": True,\n",
    "    \"gtrxl_bias\": 0.0\n",
    "}\n",
    "config = {\n",
    "    \"n_workers\": 1,\n",
    "    \"n_mini_batch\": 1,\n",
    "    \"worker_steps\": 20,\n",
    "    \"transformer\": transformer_config\n",
    "}\n",
    "\n",
    "trnsxl = Transformer(transformer_config, transformer_config[\"embed_dim\"], 20).to(device)\n",
    "print(trnsxl)\n",
    "policy_s = Policy_S(HIDDEN_DIM_DENSE, HIDDEN_DIM_DENSE, OUTPUT_DIM).to(device)\n",
    "policy_n = Policy_N(HIDDEN_DIM_DENSE, HIDDEN_DIM_DENSE, MAX_K).to(device)\n",
    "policy_c = Policy_C(HIDDEN_DIM_DENSE, HIDDEN_DIM_DENSE, LABEL_DIM).to(device)\n",
    "value_net = ValueNetwork(HIDDEN_DIM_DENSE, HIDDEN_DIM_DENSE, OUTPUT_DIM).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up optimiser\n",
    "params_pg = list(policy_s.parameters()) + list(policy_c.parameters()) + list(policy_n.parameters())\n",
    "optim_loss = optim.Adam(trnsxl.parameters(), lr=learning_rate)\n",
    "optim_policy = optim.Adam(params_pg, lr=learning_rate)\n",
    "optim_value = optim.Adam(value_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode(policy_loss_sum, encoder_loss_sum, baseline_value_batch):\n",
    "    '''\n",
    "    Called when a data sample has been processed.\n",
    "    '''\n",
    "    baseline_value_sum = torch.stack(baseline_value_batch).sum()\n",
    "    policy_loss = torch.stack(policy_loss_sum).mean()\n",
    "    encoder_loss = torch.stack(encoder_loss_sum).mean()\n",
    "    objective_loss = encoder_loss - policy_loss + baseline_value_sum\n",
    "    # set gradient to zero\n",
    "    optim_loss.zero_grad()\n",
    "    optim_policy.zero_grad()\n",
    "    optim_value.zero_grad()\n",
    "    # back propagation\n",
    "    objective_loss.backward()\n",
    "    # gradient update\n",
    "    optim_loss.step()\n",
    "    optim_policy.step()\n",
    "    optim_value.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_length = transformer_config[\"memory_length\"]\n",
    "num_blocks = transformer_config[\"num_blocks\"]\n",
    "embed_dim = transformer_config[\"embed_dim\"]\n",
    "trns_input_dim = transformer_config[\"trns_input_dim\"]\n",
    "max_episode_length = 20\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_index_select(input, dim, index):\n",
    "    \"\"\"\n",
    "    Selects values from the input tensor at the given indices along the given dimension.\n",
    "    This function is similar to torch.index_select, but it supports batched indices.\n",
    "    The input tensor is expected to be of shape (batch_size, ...), where ... means any number of additional dimensions.\n",
    "    The indices tensor is expected to be of shape (batch_size, num_indices), where num_indices is the number of indices to select for each element in the batch.\n",
    "    The output tensor is of shape (batch_size, num_indices, ...), where ... means any number of additional dimensions that were present in the input tensor.\n",
    "\n",
    "    Arguments:\n",
    "        input {torch.tensor} -- Input tensor\n",
    "        dim {int} -- Dimension along which to select values\n",
    "        index {torch.tensor} -- Tensor containing the indices to select\n",
    "\n",
    "    Returns:\n",
    "        {torch.tensor} -- Output tensor\n",
    "    \"\"\"\n",
    "    for ii in range(1, len(input.shape)):\n",
    "        if ii != dim:\n",
    "            index = index.unsqueeze(ii)\n",
    "    expanse = list(input.shape)\n",
    "    expanse[0] = -1\n",
    "    expanse[dim] = -1\n",
    "    index = index.expand(expanse)\n",
    "    return torch.gather(input, dim, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts...\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======>  torch.Size([1, 20]) torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Convolution.cpp:1004.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "  0%|          | 0/20000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m text_input \u001b[39m=\u001b[39m text[curr_step] \u001b[39m# text_input 1*20\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m# print(f\"input text_input: {text_input.shape}, memory: {memory.shape}\")\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m buffer[\u001b[39m\"\u001b[39m\u001b[39mmemory_mask\u001b[39m\u001b[39m\"\u001b[39m][:, curr_step] \u001b[39m=\u001b[39m memory_mask[torch\u001b[39m.\u001b[39;49mclip(worker_current_episode_step, \u001b[39m0\u001b[39;49m, memory_length \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m)]\n\u001b[0;32m     62\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m======> \u001b[39m\u001b[39m\"\u001b[39m, buffer[\u001b[39m\"\u001b[39m\u001b[39mmemory_indices\u001b[39m\u001b[39m\"\u001b[39m][:, curr_step]\u001b[39m.\u001b[39mshape, memory_indices[worker_current_episode_step]\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     63\u001b[0m buffer[\u001b[39m\"\u001b[39m\u001b[39mmemory_indices\u001b[39m\u001b[39m\"\u001b[39m][:, curr_step] \u001b[39m=\u001b[39m memory_indices[worker_current_episode_step]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "print('Training starts...')\n",
    "worker_current_episode_step = torch.zeros((num_workers, ), dtype=torch.long)\n",
    "for epoch in range(5):\n",
    "    print('\\nEpoch', epoch+1)\n",
    "    # log the start time of the epoch\n",
    "    start = time.time()\n",
    "    # set the models in training mode\n",
    "    trnsxl.train()\n",
    "    policy_s.train()\n",
    "    policy_n.train()\n",
    "    policy_c.train()\n",
    "    # reset the count of reread_or_skim_times\n",
    "    reread_or_skim_times = 0\n",
    "    policy_loss_sum = []\n",
    "    encoder_loss_sum = []\n",
    "    baseline_value_batch = []\n",
    "    pbar = tqdm(train_loader)\n",
    "    cm = np.zeros((LABEL_DIM, LABEL_DIM))\n",
    "   \n",
    "    for index, (x, y) in enumerate(pbar):\n",
    "        label = y.to(device)\n",
    "        text = x.to(device).view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE) # transform 1*400 to 20*1*20\n",
    "        curr_step = 0  # the position of the current chunk\n",
    "       \n",
    "        # Setup placeholders for each worker's current episodic memory\n",
    "        memory = torch.zeros((num_workers, memory_length, num_blocks, trns_input_dim), dtype=torch.float32).to(device)\n",
    "        # Generate episodic memory mask used in attention\n",
    "        memory_mask = torch.tril(torch.ones((num_workers, memory_length)), diagonal=-1).to(device)\n",
    "\n",
    "        \"\"\" e.g. memory mask tensor looks like this if memory_length = 6\n",
    "        0, 0, 0, 0, 0, 0\n",
    "        1, 0, 0, 0, 0, 0\n",
    "        1, 1, 0, 0, 0, 0\n",
    "        1, 1, 1, 0, 0, 0\n",
    "        1, 1, 1, 1, 0, 0\n",
    "        1, 1, 1, 1, 1, 0\n",
    "        \"\"\"         \n",
    "        # setup buffer for memory\n",
    "        buffer = {\"memories\": [memory[w] for w in range(num_workers)], \n",
    "                  \"memory_mask\": torch.zeros((num_workers, memory_length, memory_length), dtype=torch.bool).to(device),\n",
    "                  \"memory_index\": torch.zeros((num_workers, memory_length), dtype=torch.long).to(device), \n",
    "                  \"memory_indices\": torch.zeros((num_workers, memory_length, memory_length), dtype=torch.long).to(device)}\n",
    "        # Setup memory window indices to support a sliding window over the episodic memory\n",
    "        repetitions = torch.repeat_interleave(torch.arange(0, memory_length).unsqueeze(0), memory_length - 1, dim = 0).long()\n",
    "        memory_indices = torch.stack([torch.arange(i, i + memory_length) for i in range(max_episode_length - memory_length + 1)]).long()\n",
    "        memory_indices = torch.cat((repetitions, memory_indices)).to(device)\n",
    "        count = 0  # maximum skim/reread time: 5\n",
    "        baseline_value_ep = []\n",
    "        saved_log_probs = []  # for the use of policy gradient update\n",
    "        # collect the computational costs for every time step\n",
    "        cost_ep = []\n",
    "        # torch.autograd.set_detect_anomaly(True)\n",
    "        while curr_step < CHUNCK_SIZE and count < 5: \n",
    "            # Loop until a text can be classified or currstep is up to 20 or count reach the maximum i.e. 5.\n",
    "            # update count\n",
    "            count += 1\n",
    "            # pass the input through cnn-lstm and policy s\n",
    "            text_input = text[curr_step] # text_input 1*20\n",
    "            # print(f\"input text_input: {text_input.shape}, memory: {memory.shape}\")\n",
    "            # with torch.no_grad():\n",
    "            buffer[\"memory_mask\"][:, curr_step] = memory_mask[torch.clip(worker_current_episode_step, 0, memory_length - 1)]\n",
    "            print(\"======> \", buffer[\"memory_indices\"][:, curr_step].shape, memory_indices[worker_current_episode_step].shape)\n",
    "            buffer[\"memory_indices\"][:, curr_step] = memory_indices[worker_current_episode_step]\n",
    "            # Retrieve the memory window from the entire episodic memory\n",
    "            sliced_memory = batched_index_select(memory, 1, buffer[\"memory_indices\"][:,curr_step]).to(device)\n",
    "            ht, memory_t = trnsxl(text_input, sliced_memory, buffer[\"memory_mask\"][:, curr_step], buffer[\"memory_indices\"][:,curr_step])  # text_input: CHUNK_SIZE * 10 (10: conv out filters), memory: num_workers, CHUNK_SIZE, num_blocks, CHUNK_SIZE*10\n",
    "            # print(f\"ht memory: {ht.shape, memory_t.shape}\") # memory: num_workers, num_blocks, CHUNK_SIZe*10\n",
    "            # separate the value which is the input of value net\n",
    "            ht_ = ht.clone().detach().requires_grad_(True)\n",
    "            memory[:, worker_current_episode_step] = memory_t # .clone().detach().requires_grad_(False)\n",
    "            # print(f\"After memory update: {memory.shape}\")\n",
    "            # ht_ = ht_.view(1, ht_.shape[0] * ht_.shape[2]) # ht_: 1, NUM_RNN_LAYERS * HIDDEN_DIM_LSTM\n",
    "            # compute a baseline value for the value network\n",
    "            bi = value_net(ht_)\n",
    "            # NUM_RNN_LAYERS * 1 * 128, next input of lstm\n",
    "            # draw a stop decision\n",
    "            stop_decision, log_prob_s = sample_policy_s(ht, policy_s)\n",
    "            stop_decision = stop_decision.item()\n",
    "            if stop_decision == 1: # classify\n",
    "                worker_current_episode_step[0] = 0\n",
    "                break\n",
    "            else: \n",
    "                reread_or_skim_times += 1\n",
    "                worker_current_episode_step[0] += 1\n",
    "                # draw an action (reread or skip)\n",
    "                step, log_prob_n = sample_policy_n(ht, policy_n)\n",
    "                curr_step += int(step)  # reread or skip\n",
    "                if curr_step < CHUNCK_SIZE and count < 5:\n",
    "                    # If the code can still execute the next loop, it is not the last time step.\n",
    "                    cost_ep.append(clstm_cost + s_cost + n_cost)\n",
    "                    # add the baseline value\n",
    "                    baseline_value_ep.append(bi)\n",
    "                    # add the log prob for the current actions\n",
    "                    saved_log_probs.append(log_prob_s + log_prob_n)\n",
    "        \n",
    "        # draw a predicted label\n",
    "        output_c = policy_c(ht)\n",
    "        # cross entrpy loss input shape: input(N, C), target(N)\n",
    "        loss = criterion(output_c, label)  # positive value\n",
    "        # draw a predicted label \n",
    "        pred_label, log_prob_c = sample_policy_c(output_c)\n",
    "        # update the confusion matrix\n",
    "        cm[pred_label][y] += 1\n",
    "        if stop_decision == 1:\n",
    "            # add the cost of the last time step\n",
    "            cost_ep.append(clstm_cost + s_cost + c_cost)\n",
    "            saved_log_probs.append(log_prob_s + log_prob_c)\n",
    "        else:\n",
    "            # add the cost of the last time step\n",
    "            cost_ep.append(clstm_cost + s_cost + c_cost + n_cost)\n",
    "            # At the moment, the probability of drawing a stop decision is 1,\n",
    "            # so its log probability is zero which can be ignored in th sum.\n",
    "            saved_log_probs.append(log_prob_c.unsqueeze(0))\n",
    "        # add the baseline value\n",
    "        baseline_value_ep.append(bi)\n",
    "        # add the cross entropy loss\n",
    "        encoder_loss_sum.append(loss)\n",
    "        # compute the policy losses and value losses for the current episode\n",
    "        policy_loss_ep, value_losses = compute_policy_value_losses(cost_ep, loss, saved_log_probs, baseline_value_ep, alpha, gamma)\n",
    "        policy_loss_sum.append(torch.cat(policy_loss_ep).sum())\n",
    "        baseline_value_batch.append(torch.cat(value_losses).sum())\n",
    "        # update gradients\n",
    "        if (index + 1) % 32 == 0:  # take the average of samples, backprop\n",
    "            finish_episode(policy_loss_sum, encoder_loss_sum, baseline_value_batch)\n",
    "            del policy_loss_sum[:], encoder_loss_sum[:], baseline_value_batch[:]\n",
    "\n",
    "        # log and print out info     \n",
    "        if (index + 1) % 32 == 0:\n",
    "            stats = calculate_stats_from_cm(cm)\n",
    "            cm = np.zeros((LABEL_DIM, LABEL_DIM))\n",
    "            acc = stats[\"accuracy\"]\n",
    "            recall = stats[\"recall\"]\n",
    "            precision = stats[\"precision\"]\n",
    "            f1 = stats[\"f1\"]\n",
    "            writer.add_scalar(\"train_accuracy\", acc, len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_recall\", recall,  len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_precision\", precision,  len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_f1\", f1,  len(train_loader)*epoch + index)\n",
    "            pbar.set_description(f\"episode: {index + 1}, reread_or_skim_times: {reread_or_skim_times}, accuracy: {acc:.3f}, precision: {precision:.3f}, recall: {recall:.3f}, f1: {f1:.2f}\")\n",
    "            \n",
    "            \"\"\"print(f'\\n current episode: {index + 1}')\n",
    "            # log the current position of the text which the agent has gone through\n",
    "            print('curr_step: ', curr_step)\n",
    "            # log the sum of the rereading and skimming times\n",
    "            print(f'current reread_or_skim_times: {reread_or_skim_times}')\"\"\"\n",
    "\n",
    "\n",
    "    print('Epoch time elapsed: %.2f s' % (time.time() - start))\n",
    "    print('reread_or_skim_times in this epoch:', reread_or_skim_times)\n",
    "    count_all, count_correct = evaluate_transformer(trnsxl, policy_s, policy_n, policy_c, valid_loader)\n",
    "    print('Epoch: %s, Accuracy on the validation set: %.2f' % (epoch + 1, count_correct / count_all))\n",
    "    writer.add_scalar(\"validation_acccuracy\", count_correct / count_all,  len(train_loader)*epoch + index)\n",
    "    # count_all, count_correct = evaluate(clstm, policy_s, policy_n, policy_c, train_loader)\n",
    "    # print('Epoch: %s, Accuracy on the training set: %.2f' % (epoch + 1, count_correct / count_all))\n",
    "    \n",
    "print('Compute the accuracy on the testing set...')\n",
    "count_all, count_correct = evaluate_transformer(trnsxl, policy_s, policy_n, policy_c, test_loader)\n",
    "print('Accuracy on the testing set: %.2f' % (count_correct / count_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training starts...')\n",
    "\n",
    "for epoch in range(5, 15):\n",
    "    print('\\nEpoch', epoch+1)\n",
    "    # log the start time of the epoch\n",
    "    start = time.time()\n",
    "    # set the models in training mode\n",
    "    trnsxl.train()\n",
    "    policy_s.train()\n",
    "    policy_n.train()\n",
    "    policy_c.train()\n",
    "    # reset the count of reread_or_skim_times\n",
    "    reread_or_skim_times = 0\n",
    "    policy_loss_sum = []\n",
    "    encoder_loss_sum = []\n",
    "    baseline_value_batch = []\n",
    "    pbar = tqdm(train_loader)\n",
    "    cm = np.zeros((LABEL_DIM, LABEL_DIM))\n",
    "    for index, (x, y) in enumerate(pbar):\n",
    "        label = y.to(device)\n",
    "        text = x.to(device).view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE) # transform 1*400 to 20*1*20\n",
    "        curr_step = 0  # the position of the current chunk\n",
    "        # Setup placeholders for each worker's current episodic memory\n",
    "        memory = torch.zeros((num_workers, memory_length, num_blocks, trns_input_dim), dtype=torch.float32).to(device)\n",
    "        # Generate episodic memory mask used in attention\n",
    "        memory_mask = torch.tril(torch.ones((num_workers, memory_length)), diagonal=-1).to(device)\n",
    "        \"\"\" e.g. memory mask tensor looks like this if memory_length = 6\n",
    "        0, 0, 0, 0, 0, 0\n",
    "        1, 0, 0, 0, 0, 0\n",
    "        1, 1, 0, 0, 0, 0\n",
    "        1, 1, 1, 0, 0, 0\n",
    "        1, 1, 1, 1, 0, 0\n",
    "        1, 1, 1, 1, 1, 0\n",
    "        \"\"\"         \n",
    "        # Setup memory window indices to support a sliding window over the episodic memory\n",
    "        repetitions = torch.repeat_interleave(torch.arange(0, memory_length).unsqueeze(0), memory_length - 1, dim = 0).long()\n",
    "        memory_indices = torch.stack([torch.arange(i, i + memory_length) for i in range(max_episode_length - memory_length + 1)]).long()\n",
    "        memory_indices = torch.cat((repetitions, memory_indices)).to(device)\n",
    "        count = 0  # maximum skim/reread time: 5\n",
    "        baseline_value_ep = []\n",
    "        saved_log_probs = []  # for the use of policy gradient update\n",
    "        # collect the computational costs for every time step\n",
    "        cost_ep = []\n",
    "        with torch.no_grad():\n",
    "            while curr_step < CHUNCK_SIZE and count < 5: \n",
    "                # Loop until a text can be classified or currstep is up to 20 or count reach the maximum i.e. 5.\n",
    "                # update count\n",
    "                count += 1\n",
    "                # pass the input through cnn-lstm and policy s\n",
    "                text_input = text[curr_step] # text_input 1*20\n",
    "                # print(f\"input text_input: {text_input.shape}, memory: {memory.shape}\")\n",
    "                ht, memory_t = trnsxl(text_input, memory, memory_mask, memory_indices)  # text_input: CHUNK_SIZE * 10 (10: conv out filters), memory: num_workers, CHUNK_SIZE, num_blocks, CHUNK_SIZE*10\n",
    "                # print(f\"ht memory: {ht.shape, memory_t.shape}\") # memory: num_workers, num_blocks, CHUNK_SIZe*10\n",
    "                # separate the value which is the input of value net\n",
    "                ht_ = ht.clone().detach().requires_grad_(True)\n",
    "                memory[:, curr_step, :, :] = memory_t.clone()\n",
    "                # ht_ = ht_.view(1, ht_.shape[0] * ht_.shape[2]) # ht_: 1, NUM_RNN_LAYERS * HIDDEN_DIM_LSTM\n",
    "                # compute a baseline value for the value network\n",
    "                bi = value_net(ht_)\n",
    "                # NUM_RNN_LAYERS * 1 * 128, next input of lstm\n",
    "                # draw a stop decision\n",
    "                stop_decision, log_prob_s = sample_policy_s(ht, policy_s)\n",
    "                stop_decision = stop_decision.item()\n",
    "                if stop_decision == 1: # classify\n",
    "                    break\n",
    "                else: \n",
    "                    reread_or_skim_times += 1\n",
    "                    # draw an action (reread or skip)\n",
    "                    step, log_prob_n = sample_policy_n(ht, policy_n)\n",
    "                    curr_step += int(step)  # reread or skip\n",
    "                    if curr_step < CHUNCK_SIZE and count < 5:\n",
    "                        # If the code can still execute the next loop, it is not the last time step.\n",
    "                        cost_ep.append(clstm_cost + s_cost + n_cost)\n",
    "                        # add the baseline value\n",
    "                        baseline_value_ep.append(bi)\n",
    "                        # add the log prob for the current actions\n",
    "                        saved_log_probs.append(log_prob_s + log_prob_n)\n",
    "            \n",
    "        # draw a predicted label\n",
    "        output_c = policy_c(ht)\n",
    "        # cross entrpy loss input shape: input(N, C), target(N)\n",
    "        loss = criterion(output_c, label)  # positive value\n",
    "        # draw a predicted label \n",
    "        pred_label, log_prob_c = sample_policy_c(output_c)\n",
    "        # update the confusion matrix\n",
    "        cm[pred_label][y] += 1\n",
    "        if stop_decision == 1:\n",
    "            # add the cost of the last time step\n",
    "            cost_ep.append(clstm_cost + s_cost + c_cost)\n",
    "            saved_log_probs.append(log_prob_s + log_prob_c)\n",
    "        else:\n",
    "            # add the cost of the last time step\n",
    "            cost_ep.append(clstm_cost + s_cost + c_cost + n_cost)\n",
    "            # At the moment, the probability of drawing a stop decision is 1,\n",
    "            # so its log probability is zero which can be ignored in th sum.\n",
    "            saved_log_probs.append(log_prob_c.unsqueeze(0))\n",
    "        # add the baseline value\n",
    "        baseline_value_ep.append(bi)\n",
    "        # add the cross entropy loss\n",
    "        encoder_loss_sum.append(loss)\n",
    "        # compute the policy losses and value losses for the current episode\n",
    "        policy_loss_ep, value_losses = compute_policy_value_losses(cost_ep, loss, saved_log_probs, baseline_value_ep, alpha, gamma)\n",
    "        policy_loss_sum.append(torch.cat(policy_loss_ep).sum())\n",
    "        baseline_value_batch.append(torch.cat(value_losses).sum())\n",
    "        # update gradients\n",
    "        if (index + 1) % 32 == 0:  # take the average of samples, backprop\n",
    "            finish_episode(policy_loss_sum, encoder_loss_sum, baseline_value_batch)\n",
    "            del policy_loss_sum[:], encoder_loss_sum[:], baseline_value_batch[:]\n",
    "\n",
    "        # log and print out info     \n",
    "        if (index + 1) % 32 == 0:\n",
    "            stats = calculate_stats_from_cm(cm)\n",
    "            cm = np.zeros((LABEL_DIM, LABEL_DIM))\n",
    "            acc = stats[\"accuracy\"]\n",
    "            recall = stats[\"recall\"]\n",
    "            precision = stats[\"precision\"]\n",
    "            f1 = stats[\"f1\"]\n",
    "            writer.add_scalar(\"train_accuracy\", acc, len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_recall\", recall,  len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_precision\", precision,  len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_f1\", f1,  len(train_loader)*epoch + index)\n",
    "            pbar.set_description(f\"episode: {index + 1}, reread_or_skim_times: {reread_or_skim_times}, accuracy: {acc:.3f}, precision: {precision:.3f}, recall: {recall:.3f}, f1: {f1:.2f}\")\n",
    "            \n",
    "            \"\"\"print(f'\\n current episode: {index + 1}')\n",
    "            # log the current position of the text which the agent has gone through\n",
    "            print('curr_step: ', curr_step)\n",
    "            # log the sum of the rereading and skimming times\n",
    "            print(f'current reread_or_skim_times: {reread_or_skim_times}')\"\"\"\n",
    "\n",
    "\n",
    "    print('Epoch time elapsed: %.2f s' % (time.time() - start))\n",
    "    print('reread_or_skim_times in this epoch:', reread_or_skim_times)\n",
    "    count_all, count_correct = evaluate_transformer(trnsxl, policy_s, policy_n, policy_c, valid_loader)\n",
    "    print('Epoch: %s, Accuracy on the validation set: %.2f' % (epoch + 1, count_correct / count_all))\n",
    "    writer.add_scalar(\"validation_acccuracy\", count_correct / count_all,  len(train_loader)*epoch + index)\n",
    "    # count_all, count_correct = evaluate(clstm, policy_s, policy_n, policy_c, train_loader)\n",
    "    # print('Epoch: %s, Accuracy on the training set: %.2f' % (epoch + 1, count_correct / count_all))\n",
    "    \n",
    "print('Compute the accuracy on the testing set...')\n",
    "count_all, count_correct = evaluate_transformer(trnsxl, policy_s, policy_n, policy_c, test_loader)\n",
    "print('Accuracy on the testing set: %.2f' % (count_correct / count_all))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_all, count_correct = evaluate_transformer(trnsxl, policy_s, policy_n, policy_c, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_correct / count_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "now_time = str(now.day) + \"_\" + str(now.month) + \"_\" + str(now.year) + \"_\" + str(now.hour) + \"_\" + str(now.minute)\n",
    "now_date = str(now.day) + \"_\" + str(now.month) + \"_\" + str(now.year)\n",
    "now_date, now_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_models(time_str, date_str, path: str = \".\"):\n",
    "    os.makedirs(f\"saved_models\\\\{date_str}\", exist_ok=True)\n",
    "    torch.save(clstm, f\"{date_str}\\\\clstm_{time_str}.pth\")\n",
    "    torch.save(policy_s, f\"{date_str}\\\\policy_s_{time_str}.pth\")\n",
    "    torch.save(policy_n, f\"{date_str}\\\\policy_n_{time_str}.pth\")\n",
    "    torch.save(policy_c, f\"{date_str}\\\\policy_c_{time_str}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models(now_time, now_date, \"saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clstm.eval()\n",
    "policy_s.eval()\n",
    "policy_n.eval()\n",
    "policy_c.eval()\n",
    "action_logs = []\n",
    "seen_logs = []\n",
    "writer = SummaryWriter()\n",
    "for i, (x, y) in enumerate(valid_loader):\n",
    "    print(i)\n",
    "    print(valid_data[\"text\"].iloc[i])\n",
    "    print(y)\n",
    "    action_log_batch = []\n",
    "    seen_batch = []\n",
    "    label = y.to(device).long() # for cross entropy loss, the long type is required\n",
    "    text = x.to(device).view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE) # transform 1*400 to 20*1*20\n",
    "    curr_step = 0\n",
    "    n_rnn_layers = clstm.n_rnn_layers\n",
    "    lstm_hidden_dim = clstm.lstm_hidden_dim\n",
    "    h_0 = torch.zeros([n_rnn_layers,1,lstm_hidden_dim]).to(device)\n",
    "    c_0 = torch.zeros([n_rnn_layers,1,lstm_hidden_dim]).to(device)\n",
    "    count = 0\n",
    "    while curr_step < 20 and count < 5: # loop until a text can be classified or currstep is up to 20\n",
    "        count += 1\n",
    "        # pass the input through cnn-lstm and policy s\n",
    "        text_input = text[curr_step] # text_input 1*20\n",
    "        text_str = train_data[\"text\"].iloc[i]\n",
    "        seen_batch.append(text_str.split()[curr_step * 20: (curr_step+1)*20])\n",
    "        ht, ct = clstm(text_input, h_0, c_0)  # 1 * 128\n",
    "        # if count == 1 and i == 0:\n",
    "        #     writer.add_graph(clstm, [text_input, h_0, c_0], verbose=True)\n",
    "        h_0 = ht.unsqueeze(0) # NUM_RNN_LAYERS * 1 * LSTM_HIDDEN_DIM, next input of lstm\n",
    "        c_0 = ct\n",
    "        # ht_ = ht.view(1, ht.shape[0] * ht.shape[2])\n",
    "        # draw a stop decision\n",
    "        stop_decision, log_prob_s = sample_policy_s(ht, policy_s)\n",
    "        # if count == 1 and i == 1:\n",
    "        #     writer.add_graph(policy_s, ht)\n",
    "        stop_decision = stop_decision.item()\n",
    "        if stop_decision == 1: # classify\n",
    "            break\n",
    "        else:\n",
    "            # draw an action (reread or skip)\n",
    "            step, log_prob_n = sample_policy_n(ht, policy_n)\n",
    "            # if count == 1 and i == 2:\n",
    "            #     writer.add_graph(policy_n, ht)\n",
    "            curr_step += int(step)  # reread or skip\n",
    "            action_log_batch.append({\"skip/reread\": step})\n",
    "    # draw a predicted label\n",
    "    output_c = policy_c(ht)\n",
    "    if i == 3:\n",
    "        writer.add_graph(policy_c, ht, verbose=True)\n",
    "        \n",
    "    # draw a predicted label \n",
    "    pred_label, log_prob_c = sample_policy_c(output_c)\n",
    "    action_log_batch.append({\"prediction\": pred_label, \"real\": label})\n",
    "    if pred_label.item() == label:\n",
    "        count_correct += 1\n",
    "    count_all += 1\n",
    "    action_logs.append(action_log_batch)\n",
    "    seen_logs.append(seen_batch)\n",
    "    if i == 10:\n",
    "        break\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_logs[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 3\n",
    "action_logs[ix], \" \".join(train_data[\"text\"].iloc[ix].split()[0:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(seen_logs[ix][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_correct / count_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
