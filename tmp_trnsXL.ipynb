{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Bernoulli, Categorical\n",
    "from torchtext import datasets\n",
    "import os\n",
    "import time\n",
    "import numpy as np \n",
    "import random\n",
    "import argparse\n",
    "\n",
    "from networks import Policy_C, Policy_N, Policy_S, ValueNetwork, Transformer\n",
    "from utils.utils import sample_policy_c, sample_policy_n, sample_policy_s, evaluate_transformer, compute_policy_value_losses\n",
    "from utils.utils import cnn_cost, clstm_cost, c_cost, n_cost, s_cost, openDfFromPickle, calculate_stats_from_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Number of training examples: 20000\n",
      "Number of validation examples: 5000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print('Reading data...')\n",
    "train_data = openDfFromPickle(\"C:\\\\Users\\\\mrbal\\\\Documents\\\\NLP\\\\RL\\\\basic_reinforcement_learning\\\\NLP_datasets\\\\imdb\\\\imdb_train_distilbert-base-uncased.pkl\")\n",
    "valid_data = openDfFromPickle(\"C:\\\\Users\\\\mrbal\\\\Documents\\\\NLP\\\\RL\\\\basic_reinforcement_learning\\\\NLP_datasets\\\\imdb\\\\imdb_val_distilbert-base-uncased.pkl\")\n",
    "test_data = openDfFromPickle(\"C:\\\\Users\\\\mrbal\\\\Documents\\\\NLP\\\\RL\\\\basic_reinforcement_learning\\\\NLP_datasets\\\\imdb\\\\imdb_test_distilbert-base-uncased.pkl\")\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# split the datasets into batches\n",
    "BATCH_SIZE = 1  # the batch size for a dataset iterator\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20000, 400]) torch.Size([20000])\n",
      "torch.Size([5000, 400]) torch.Size([5000])\n",
      "torch.Size([25000, 400]) torch.Size([25000])\n"
     ]
    }
   ],
   "source": [
    "xtrain = torch.from_numpy(np.stack(train_data[\"text_bert_input_ids\"].values))[:, 0:400]\n",
    "ytrain = torch.from_numpy(train_data[\"label\"].values)\n",
    "xvalid = torch.from_numpy(np.stack(valid_data[\"text_bert_input_ids\"].values))[:, 0:400]\n",
    "yvalid = torch.from_numpy(valid_data[\"label\"].values)\n",
    "xtest = torch.from_numpy(np.stack(test_data[\"text_bert_input_ids\"].values))[:, 0:400]\n",
    "ytest = torch.from_numpy(test_data[\"label\"].values)\n",
    "\n",
    "print(xtrain.shape, ytrain.shape)\n",
    "print(xvalid.shape, yvalid.shape)\n",
    "print(xtest.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "train_loader = DataLoader(TensorDataset(xtrain, ytrain), batch_size=BATCH_SIZE)\n",
    "valid_loader = DataLoader(TensorDataset(xvalid, yvalid), batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(TensorDataset(xtest, ytest), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2023)\n",
    "np.random.seed(2023)\n",
    "torch.manual_seed(2023)\n",
    "torch.cuda.manual_seed(2023)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "INPUT_DIM = 30522\n",
    "CHUNCK_SIZE = 20\n",
    "EMBEDDING_DIM = 100\n",
    "NUM_RNN_LAYERS = 1\n",
    "KER_SIZE = 5\n",
    "HIDDEN_DIM_LSTM = 128 \n",
    "HIDDEN_DIM_DENSE = CHUNCK_SIZE * 10 # HIDDEN_DIM_LSTM * NUM_RNN_LAYERS\n",
    "OUTPUT_DIM = 1\n",
    "MAX_K = 4  # the output dimension for step size 0, 1, 2, 3\n",
    "LABEL_DIM = 2\n",
    "N_FILTERS = 128\n",
    "BATCH_SIZE = 1\n",
    "gamma = 0.99\n",
    "alpha = 0.2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (activation): ReLU()\n",
      "  (linear_embedding): Embedding(30528, 100)\n",
      "  (conv): Conv2d(100, 10, kernel_size=(4, 4), stride=(1, 1), padding=same)\n",
      "  (transformer_blocks): ModuleList(\n",
      "    (0-2): 3 x TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (values): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (keys): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (queries): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (fc_out): Linear(in_features=200, out_features=200, bias=True)\n",
      "      )\n",
      "      (gate1): GRUGate(\n",
      "        (Wr): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Ur): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Wz): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Uz): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Wg): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Ug): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (sigmoid): Sigmoid()\n",
      "        (tanh): Tanh()\n",
      "      )\n",
      "      (gate2): GRUGate(\n",
      "        (Wr): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Ur): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Wz): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Uz): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Wg): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (Ug): Linear(in_features=200, out_features=200, bias=False)\n",
      "        (sigmoid): Sigmoid()\n",
      "        (tanh): Tanh()\n",
      "      )\n",
      "      (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_kv): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# set up the criterion\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "# set up models\n",
    "transformer_config = {\n",
    "    \"num_blocks\": 3,\n",
    "    \"embed_dim\": 100, \n",
    "    \"trns_input_dim\": 20*10, # embedding dim (per word) * chunk_size\n",
    "    \"num_heads\": 1,\n",
    "    \"memory_length\": 20,\n",
    "    \"positional_encoding\": \"\", # options: \"\" \"relative\" \"learned\"\n",
    "    \"layer_norm\": \"pre\", # options: \"\" \"pre\" \"post\"\n",
    "    \"gtrxl\": True,\n",
    "    \"gtrxl_bias\": 0.0\n",
    "}\n",
    "trnsxl = Transformer(transformer_config, transformer_config[\"embed_dim\"], 20).to(device)\n",
    "print(trnsxl)\n",
    "policy_s = Policy_S(HIDDEN_DIM_DENSE, HIDDEN_DIM_DENSE, OUTPUT_DIM).to(device)\n",
    "policy_n = Policy_N(HIDDEN_DIM_DENSE, HIDDEN_DIM_DENSE, MAX_K).to(device)\n",
    "policy_c = Policy_C(HIDDEN_DIM_DENSE, HIDDEN_DIM_DENSE, LABEL_DIM).to(device)\n",
    "value_net = ValueNetwork(HIDDEN_DIM_DENSE, HIDDEN_DIM_DENSE, OUTPUT_DIM).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up optimiser\n",
    "params_pg = list(policy_s.parameters()) + list(policy_c.parameters()) + list(policy_n.parameters())\n",
    "optim_loss = optim.Adam(trnsxl.parameters(), lr=learning_rate)\n",
    "optim_policy = optim.Adam(params_pg, lr=learning_rate)\n",
    "optim_value = optim.Adam(value_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode(policy_loss_sum, encoder_loss_sum, baseline_value_batch):\n",
    "    '''\n",
    "    Called when a data sample has been processed.\n",
    "    '''\n",
    "    baseline_value_sum = torch.stack(baseline_value_batch).sum()\n",
    "    policy_loss = torch.stack(policy_loss_sum).mean()\n",
    "    encoder_loss = torch.stack(encoder_loss_sum).mean()\n",
    "    objective_loss = encoder_loss - policy_loss + baseline_value_sum\n",
    "    # set gradient to zero\n",
    "    optim_loss.zero_grad()\n",
    "    optim_policy.zero_grad()\n",
    "    optim_value.zero_grad()\n",
    "    # back propagation\n",
    "    objective_loss.backward()\n",
    "    # gradient update\n",
    "    optim_loss.step()\n",
    "    optim_policy.step()\n",
    "    optim_value.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_length = transformer_config[\"memory_length\"]\n",
    "num_blocks = transformer_config[\"num_blocks\"]\n",
    "embed_dim = transformer_config[\"embed_dim\"]\n",
    "trns_input_dim = transformer_config[\"trns_input_dim\"]\n",
    "max_episode_length = 20\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts...\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 31/20000 [00:45<8:04:06,  1.45s/it]c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: Error detected in NativeLayerNormBackward0. Traceback of forward call that caused the error:\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 711, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 411, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 531, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\mrbal\\AppData\\Local\\Temp\\ipykernel_13732\\1934167998.py\", line 52, in <module>\n",
      "    ht, memory_t = trnsxl(text_input, memory, memory_mask, memory_indices)  # text_input: CHUNK_SIZE * 10 (10: conv out filters), memory: num_workers, CHUNK_SIZE, num_blocks, CHUNK_SIZE*10\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\mrbal\\Documents\\NLP\\RL\\reproduction_sou\\Fast-And-Accurate-Text-Classification-Reproduction\\networks.py\", line 476, in forward\n",
      "    h, attention_weights = block(memories[:, :, i], memories[:, :, i], h.unsqueeze(1), mask) # args: value, key, query, mask\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\mrbal\\Documents\\NLP\\RL\\reproduction_sou\\Fast-And-Accurate-Text-Classification-Reproduction\\networks.py\", line 339, in forward\n",
      "    value = self.norm_kv(value)\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\torch\\nn\\modules\\normalization.py\", line 190, in forward\n",
      "    return F.layer_norm(\n",
      "  File \"c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\torch\\nn\\functional.py\", line 2515, in layer_norm\n",
      "    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\n",
      " (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:119.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "  0%|          | 31/20000 [00:46<8:23:56,  1.51s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [1, 20, 200]] is at version 5; expected version 4 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 108\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39m# update gradients\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[39mif\u001b[39;00m (index \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m32\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:  \u001b[39m# take the average of samples, backprop\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     finish_episode(policy_loss_sum, encoder_loss_sum, baseline_value_batch)\n\u001b[0;32m    109\u001b[0m     \u001b[39mdel\u001b[39;00m policy_loss_sum[:], encoder_loss_sum[:], baseline_value_batch[:]\n\u001b[0;32m    111\u001b[0m \u001b[39m# log and print out info     \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m, in \u001b[0;36mfinish_episode\u001b[1;34m(policy_loss_sum, encoder_loss_sum, baseline_value_batch)\u001b[0m\n\u001b[0;32m     12\u001b[0m optim_value\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     13\u001b[0m \u001b[39m# back propagation\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m objective_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     15\u001b[0m \u001b[39m# gradient update\u001b[39;00m\n\u001b[0;32m     16\u001b[0m optim_loss\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [1, 20, 200]] is at version 5; expected version 4 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "print('Training starts...')\n",
    "\n",
    "for epoch in range(5):\n",
    "    print('\\nEpoch', epoch+1)\n",
    "    # log the start time of the epoch\n",
    "    start = time.time()\n",
    "    # set the models in training mode\n",
    "    trnsxl.train()\n",
    "    policy_s.train()\n",
    "    policy_n.train()\n",
    "    policy_c.train()\n",
    "    # reset the count of reread_or_skim_times\n",
    "    reread_or_skim_times = 0\n",
    "    policy_loss_sum = []\n",
    "    encoder_loss_sum = []\n",
    "    baseline_value_batch = []\n",
    "    pbar = tqdm(train_loader)\n",
    "    cm = np.zeros((LABEL_DIM, LABEL_DIM))\n",
    "    for index, (x, y) in enumerate(pbar):\n",
    "        label = y.to(device)\n",
    "        text = x.to(device).view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE) # transform 1*400 to 20*1*20\n",
    "        curr_step = 0  # the position of the current chunk\n",
    "        # Setup placeholders for each worker's current episodic memory\n",
    "        memory = torch.zeros((num_workers, memory_length, num_blocks, trns_input_dim), dtype=torch.float32).to(device)\n",
    "        # Generate episodic memory mask used in attention\n",
    "        memory_mask = torch.tril(torch.ones((num_workers, memory_length)), diagonal=-1).to(device)\n",
    "        \"\"\" e.g. memory mask tensor looks like this if memory_length = 6\n",
    "        0, 0, 0, 0, 0, 0\n",
    "        1, 0, 0, 0, 0, 0\n",
    "        1, 1, 0, 0, 0, 0\n",
    "        1, 1, 1, 0, 0, 0\n",
    "        1, 1, 1, 1, 0, 0\n",
    "        1, 1, 1, 1, 1, 0\n",
    "        \"\"\"         \n",
    "        # Setup memory window indices to support a sliding window over the episodic memory\n",
    "        repetitions = torch.repeat_interleave(torch.arange(0, memory_length).unsqueeze(0), memory_length - 1, dim = 0).long()\n",
    "        memory_indices = torch.stack([torch.arange(i, i + memory_length) for i in range(max_episode_length - memory_length + 1)]).long()\n",
    "        memory_indices = torch.cat((repetitions, memory_indices)).to(device)\n",
    "        count = 0  # maximum skim/reread time: 5\n",
    "        baseline_value_ep = []\n",
    "        saved_log_probs = []  # for the use of policy gradient update\n",
    "        # collect the computational costs for every time step\n",
    "        cost_ep = []\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        while curr_step < CHUNCK_SIZE and count < 5: \n",
    "            # Loop until a text can be classified or currstep is up to 20 or count reach the maximum i.e. 5.\n",
    "            # update count\n",
    "            count += 1\n",
    "            # pass the input through cnn-lstm and policy s\n",
    "            text_input = text[curr_step] # text_input 1*20\n",
    "            # print(f\"input text_input: {text_input.shape}, memory: {memory.shape}\")\n",
    "            ht, memory_t = trnsxl(text_input, memory, memory_mask, memory_indices)  # text_input: CHUNK_SIZE * 10 (10: conv out filters), memory: num_workers, CHUNK_SIZE, num_blocks, CHUNK_SIZE*10\n",
    "            # print(f\"ht memory: {ht.shape, memory_t.shape}\") # memory: num_workers, num_blocks, CHUNK_SIZe*10\n",
    "            # separate the value which is the input of value net\n",
    "            ht_ = ht.clone().detach().requires_grad_(True)\n",
    "            memory[:, curr_step, :, :] = memory_t # .clone().detach().requires_grad_(False)\n",
    "            # print(f\"After memory update: {memory.shape}\")\n",
    "            # ht_ = ht_.view(1, ht_.shape[0] * ht_.shape[2]) # ht_: 1, NUM_RNN_LAYERS * HIDDEN_DIM_LSTM\n",
    "            # compute a baseline value for the value network\n",
    "            bi = value_net(ht_)\n",
    "            # NUM_RNN_LAYERS * 1 * 128, next input of lstm\n",
    "            # draw a stop decision\n",
    "            stop_decision, log_prob_s = sample_policy_s(ht, policy_s)\n",
    "            stop_decision = stop_decision.item()\n",
    "            if stop_decision == 1: # classify\n",
    "                break\n",
    "            else: \n",
    "                reread_or_skim_times += 1\n",
    "                # draw an action (reread or skip)\n",
    "                step, log_prob_n = sample_policy_n(ht, policy_n)\n",
    "                curr_step += int(step)  # reread or skip\n",
    "                if curr_step < CHUNCK_SIZE and count < 5:\n",
    "                    # If the code can still execute the next loop, it is not the last time step.\n",
    "                    cost_ep.append(clstm_cost + s_cost + n_cost)\n",
    "                    # add the baseline value\n",
    "                    baseline_value_ep.append(bi)\n",
    "                    # add the log prob for the current actions\n",
    "                    saved_log_probs.append(log_prob_s + log_prob_n)\n",
    "        \n",
    "        # draw a predicted label\n",
    "        output_c = policy_c(ht)\n",
    "        # cross entrpy loss input shape: input(N, C), target(N)\n",
    "        loss = criterion(output_c, label)  # positive value\n",
    "        # draw a predicted label \n",
    "        pred_label, log_prob_c = sample_policy_c(output_c)\n",
    "        # update the confusion matrix\n",
    "        cm[pred_label][y] += 1\n",
    "        if stop_decision == 1:\n",
    "            # add the cost of the last time step\n",
    "            cost_ep.append(clstm_cost + s_cost + c_cost)\n",
    "            saved_log_probs.append(log_prob_s + log_prob_c)\n",
    "        else:\n",
    "            # add the cost of the last time step\n",
    "            cost_ep.append(clstm_cost + s_cost + c_cost + n_cost)\n",
    "            # At the moment, the probability of drawing a stop decision is 1,\n",
    "            # so its log probability is zero which can be ignored in th sum.\n",
    "            saved_log_probs.append(log_prob_c.unsqueeze(0))\n",
    "        # add the baseline value\n",
    "        baseline_value_ep.append(bi)\n",
    "        # add the cross entropy loss\n",
    "        encoder_loss_sum.append(loss)\n",
    "        # compute the policy losses and value losses for the current episode\n",
    "        policy_loss_ep, value_losses = compute_policy_value_losses(cost_ep, loss, saved_log_probs, baseline_value_ep, alpha, gamma)\n",
    "        policy_loss_sum.append(torch.cat(policy_loss_ep).sum())\n",
    "        baseline_value_batch.append(torch.cat(value_losses).sum())\n",
    "        # update gradients\n",
    "        if (index + 1) % 32 == 0:  # take the average of samples, backprop\n",
    "            finish_episode(policy_loss_sum, encoder_loss_sum, baseline_value_batch)\n",
    "            del policy_loss_sum[:], encoder_loss_sum[:], baseline_value_batch[:]\n",
    "\n",
    "        # log and print out info     \n",
    "        if (index + 1) % 32 == 0:\n",
    "            stats = calculate_stats_from_cm(cm)\n",
    "            cm = np.zeros((LABEL_DIM, LABEL_DIM))\n",
    "            acc = stats[\"accuracy\"]\n",
    "            recall = stats[\"recall\"]\n",
    "            precision = stats[\"precision\"]\n",
    "            f1 = stats[\"f1\"]\n",
    "            writer.add_scalar(\"train_accuracy\", acc, len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_recall\", recall,  len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_precision\", precision,  len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_f1\", f1,  len(train_loader)*epoch + index)\n",
    "            pbar.set_description(f\"episode: {index + 1}, reread_or_skim_times: {reread_or_skim_times}, accuracy: {acc:.3f}, precision: {precision:.3f}, recall: {recall:.3f}, f1: {f1:.2f}\")\n",
    "            \n",
    "            \"\"\"print(f'\\n current episode: {index + 1}')\n",
    "            # log the current position of the text which the agent has gone through\n",
    "            print('curr_step: ', curr_step)\n",
    "            # log the sum of the rereading and skimming times\n",
    "            print(f'current reread_or_skim_times: {reread_or_skim_times}')\"\"\"\n",
    "\n",
    "\n",
    "    print('Epoch time elapsed: %.2f s' % (time.time() - start))\n",
    "    print('reread_or_skim_times in this epoch:', reread_or_skim_times)\n",
    "    count_all, count_correct = evaluate_transformer(trnsxl, policy_s, policy_n, policy_c, valid_loader)\n",
    "    print('Epoch: %s, Accuracy on the validation set: %.2f' % (epoch + 1, count_correct / count_all))\n",
    "    writer.add_scalar(\"validation_acccuracy\", count_correct / count_all,  len(train_loader)*epoch + index)\n",
    "    # count_all, count_correct = evaluate(clstm, policy_s, policy_n, policy_c, train_loader)\n",
    "    # print('Epoch: %s, Accuracy on the training set: %.2f' % (epoch + 1, count_correct / count_all))\n",
    "    \n",
    "print('Compute the accuracy on the testing set...')\n",
    "count_all, count_correct = evaluate_transformer(trnsxl, policy_s, policy_n, policy_c, test_loader)\n",
    "print('Accuracy on the testing set: %.2f' % (count_correct / count_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [1, 20, 200]] is at version 5; expected version 4 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 108\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39m# update gradients\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[39mif\u001b[39;00m (index \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m32\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:  \u001b[39m# take the average of samples, backprop\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     finish_episode(policy_loss_sum, encoder_loss_sum, baseline_value_batch)\n\u001b[0;32m    109\u001b[0m     \u001b[39mdel\u001b[39;00m policy_loss_sum[:], encoder_loss_sum[:], baseline_value_batch[:]\n\u001b[0;32m    111\u001b[0m \u001b[39m# log and print out info     \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m, in \u001b[0;36mfinish_episode\u001b[1;34m(policy_loss_sum, encoder_loss_sum, baseline_value_batch)\u001b[0m\n\u001b[0;32m     12\u001b[0m optim_value\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     13\u001b[0m \u001b[39m# back propagation\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m objective_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     15\u001b[0m \u001b[39m# gradient update\u001b[39;00m\n\u001b[0;32m     16\u001b[0m optim_loss\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [1, 20, 200]] is at version 5; expected version 4 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts...\n",
      "\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 100000, accuracy: 0.531, precision: 0.525, recall: 0.524, f1: 0.52: 100%|██████████| 20000/20000 [15:27<00:00, 21.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 927.01 s\n",
      "reread_or_skim_times in this epoch: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [03:27<00:00, 24.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 207.56 s\n",
      "Average FLOPs per sample:  7072170\n",
      "Epoch: 6, Accuracy on the validation set: 0.50\n",
      "\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 100000, accuracy: 0.438, precision: 0.420, recall: 0.427, f1: 0.42: 100%|██████████| 20000/20000 [15:02<00:00, 22.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 902.16 s\n",
      "reread_or_skim_times in this epoch: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [03:25<00:00, 24.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 205.18 s\n",
      "Average FLOPs per sample:  7072170\n",
      "Epoch: 7, Accuracy on the validation set: 0.50\n",
      "\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 100000, accuracy: 0.406, precision: 0.409, recall: 0.410, f1: 0.41: 100%|██████████| 20000/20000 [15:03<00:00, 22.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 903.08 s\n",
      "reread_or_skim_times in this epoch: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [03:28<00:00, 24.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 208.01 s\n",
      "Average FLOPs per sample:  7072170\n",
      "Epoch: 8, Accuracy on the validation set: 0.50\n",
      "\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 100000, accuracy: 0.500, precision: 0.502, recall: 0.502, f1: 0.50: 100%|██████████| 20000/20000 [15:04<00:00, 22.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 904.85 s\n",
      "reread_or_skim_times in this epoch: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [03:26<00:00, 24.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 206.58 s\n",
      "Average FLOPs per sample:  7072170\n",
      "Epoch: 9, Accuracy on the validation set: 0.49\n",
      "\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 99998, accuracy: 0.562, precision: 0.561, recall: 0.561, f1: 0.56: 100%|██████████| 20000/20000 [15:05<00:00, 22.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 905.64 s\n",
      "reread_or_skim_times in this epoch: 99998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [03:28<00:00, 24.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 208.13 s\n",
      "Average FLOPs per sample:  7072170\n",
      "Epoch: 10, Accuracy on the validation set: 0.50\n",
      "\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 99993, accuracy: 0.531, precision: 0.536, recall: 0.535, f1: 0.53: 100%|██████████| 20000/20000 [15:10<00:00, 21.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 910.75 s\n",
      "reread_or_skim_times in this epoch: 99993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [03:27<00:00, 24.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 207.49 s\n",
      "Average FLOPs per sample:  7072170\n",
      "Epoch: 11, Accuracy on the validation set: 0.50\n",
      "\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 99998, accuracy: 0.406, precision: 0.408, recall: 0.414, f1: 0.40: 100%|██████████| 20000/20000 [15:04<00:00, 22.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 904.83 s\n",
      "reread_or_skim_times in this epoch: 99998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [03:25<00:00, 24.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 205.23 s\n",
      "Average FLOPs per sample:  7072170\n",
      "Epoch: 12, Accuracy on the validation set: 0.49\n",
      "\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 100000, accuracy: 0.469, precision: 0.458, recall: 0.461, f1: 0.46: 100%|██████████| 20000/20000 [15:05<00:00, 22.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 905.44 s\n",
      "reread_or_skim_times in this epoch: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [03:27<00:00, 24.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 207.12 s\n",
      "Average FLOPs per sample:  7072170\n",
      "Epoch: 13, Accuracy on the validation set: 0.50\n",
      "\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 99996, accuracy: 0.500, precision: 0.498, recall: 0.498, f1: 0.50: 100%|██████████| 20000/20000 [15:09<00:00, 21.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 909.73 s\n",
      "reread_or_skim_times in this epoch: 99996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [03:26<00:00, 24.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 206.30 s\n",
      "Average FLOPs per sample:  7072170\n",
      "Epoch: 14, Accuracy on the validation set: 0.49\n",
      "\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20000, reread_or_skim_times: 100000, accuracy: 0.562, precision: 0.565, recall: 0.565, f1: 0.56: 100%|██████████| 20000/20000 [15:04<00:00, 22.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time elapsed: 904.29 s\n",
      "reread_or_skim_times in this epoch: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 5000/5000 [03:26<00:00, 24.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 206.48 s\n",
      "Average FLOPs per sample:  7072170\n",
      "Epoch: 15, Accuracy on the validation set: 0.50\n",
      "Compute the accuracy on the testing set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 25000/25000 [17:16<00:00, 24.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 1036.88 s\n",
      "Average FLOPs per sample:  7072170\n",
      "Accuracy on the testing set: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training starts...')\n",
    "\n",
    "for epoch in range(5, 15):\n",
    "    print('\\nEpoch', epoch+1)\n",
    "    # log the start time of the epoch\n",
    "    start = time.time()\n",
    "    # set the models in training mode\n",
    "    trnsxl.train()\n",
    "    policy_s.train()\n",
    "    policy_n.train()\n",
    "    policy_c.train()\n",
    "    # reset the count of reread_or_skim_times\n",
    "    reread_or_skim_times = 0\n",
    "    policy_loss_sum = []\n",
    "    encoder_loss_sum = []\n",
    "    baseline_value_batch = []\n",
    "    pbar = tqdm(train_loader)\n",
    "    cm = np.zeros((LABEL_DIM, LABEL_DIM))\n",
    "    for index, (x, y) in enumerate(pbar):\n",
    "        label = y.to(device)\n",
    "        text = x.to(device).view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE) # transform 1*400 to 20*1*20\n",
    "        curr_step = 0  # the position of the current chunk\n",
    "        # Setup placeholders for each worker's current episodic memory\n",
    "        memory = torch.zeros((num_workers, memory_length, num_blocks, trns_input_dim), dtype=torch.float32).to(device)\n",
    "        # Generate episodic memory mask used in attention\n",
    "        memory_mask = torch.tril(torch.ones((num_workers, memory_length)), diagonal=-1).to(device)\n",
    "        \"\"\" e.g. memory mask tensor looks like this if memory_length = 6\n",
    "        0, 0, 0, 0, 0, 0\n",
    "        1, 0, 0, 0, 0, 0\n",
    "        1, 1, 0, 0, 0, 0\n",
    "        1, 1, 1, 0, 0, 0\n",
    "        1, 1, 1, 1, 0, 0\n",
    "        1, 1, 1, 1, 1, 0\n",
    "        \"\"\"         \n",
    "        # Setup memory window indices to support a sliding window over the episodic memory\n",
    "        repetitions = torch.repeat_interleave(torch.arange(0, memory_length).unsqueeze(0), memory_length - 1, dim = 0).long()\n",
    "        memory_indices = torch.stack([torch.arange(i, i + memory_length) for i in range(max_episode_length - memory_length + 1)]).long()\n",
    "        memory_indices = torch.cat((repetitions, memory_indices)).to(device)\n",
    "        count = 0  # maximum skim/reread time: 5\n",
    "        baseline_value_ep = []\n",
    "        saved_log_probs = []  # for the use of policy gradient update\n",
    "        # collect the computational costs for every time step\n",
    "        cost_ep = []\n",
    "        with torch.no_grad():\n",
    "            while curr_step < CHUNCK_SIZE and count < 5: \n",
    "                # Loop until a text can be classified or currstep is up to 20 or count reach the maximum i.e. 5.\n",
    "                # update count\n",
    "                count += 1\n",
    "                # pass the input through cnn-lstm and policy s\n",
    "                text_input = text[curr_step] # text_input 1*20\n",
    "                # print(f\"input text_input: {text_input.shape}, memory: {memory.shape}\")\n",
    "                ht, memory_t = trnsxl(text_input, memory, memory_mask, memory_indices)  # text_input: CHUNK_SIZE * 10 (10: conv out filters), memory: num_workers, CHUNK_SIZE, num_blocks, CHUNK_SIZE*10\n",
    "                # print(f\"ht memory: {ht.shape, memory_t.shape}\") # memory: num_workers, num_blocks, CHUNK_SIZe*10\n",
    "                # separate the value which is the input of value net\n",
    "                ht_ = ht.clone().detach().requires_grad_(True)\n",
    "                memory[:, curr_step, :, :] = memory_t.clone()\n",
    "                # ht_ = ht_.view(1, ht_.shape[0] * ht_.shape[2]) # ht_: 1, NUM_RNN_LAYERS * HIDDEN_DIM_LSTM\n",
    "                # compute a baseline value for the value network\n",
    "                bi = value_net(ht_)\n",
    "                # NUM_RNN_LAYERS * 1 * 128, next input of lstm\n",
    "                # draw a stop decision\n",
    "                stop_decision, log_prob_s = sample_policy_s(ht, policy_s)\n",
    "                stop_decision = stop_decision.item()\n",
    "                if stop_decision == 1: # classify\n",
    "                    break\n",
    "                else: \n",
    "                    reread_or_skim_times += 1\n",
    "                    # draw an action (reread or skip)\n",
    "                    step, log_prob_n = sample_policy_n(ht, policy_n)\n",
    "                    curr_step += int(step)  # reread or skip\n",
    "                    if curr_step < CHUNCK_SIZE and count < 5:\n",
    "                        # If the code can still execute the next loop, it is not the last time step.\n",
    "                        cost_ep.append(clstm_cost + s_cost + n_cost)\n",
    "                        # add the baseline value\n",
    "                        baseline_value_ep.append(bi)\n",
    "                        # add the log prob for the current actions\n",
    "                        saved_log_probs.append(log_prob_s + log_prob_n)\n",
    "            \n",
    "        # draw a predicted label\n",
    "        output_c = policy_c(ht)\n",
    "        # cross entrpy loss input shape: input(N, C), target(N)\n",
    "        loss = criterion(output_c, label)  # positive value\n",
    "        # draw a predicted label \n",
    "        pred_label, log_prob_c = sample_policy_c(output_c)\n",
    "        # update the confusion matrix\n",
    "        cm[pred_label][y] += 1\n",
    "        if stop_decision == 1:\n",
    "            # add the cost of the last time step\n",
    "            cost_ep.append(clstm_cost + s_cost + c_cost)\n",
    "            saved_log_probs.append(log_prob_s + log_prob_c)\n",
    "        else:\n",
    "            # add the cost of the last time step\n",
    "            cost_ep.append(clstm_cost + s_cost + c_cost + n_cost)\n",
    "            # At the moment, the probability of drawing a stop decision is 1,\n",
    "            # so its log probability is zero which can be ignored in th sum.\n",
    "            saved_log_probs.append(log_prob_c.unsqueeze(0))\n",
    "        # add the baseline value\n",
    "        baseline_value_ep.append(bi)\n",
    "        # add the cross entropy loss\n",
    "        encoder_loss_sum.append(loss)\n",
    "        # compute the policy losses and value losses for the current episode\n",
    "        policy_loss_ep, value_losses = compute_policy_value_losses(cost_ep, loss, saved_log_probs, baseline_value_ep, alpha, gamma)\n",
    "        policy_loss_sum.append(torch.cat(policy_loss_ep).sum())\n",
    "        baseline_value_batch.append(torch.cat(value_losses).sum())\n",
    "        # update gradients\n",
    "        if (index + 1) % 32 == 0:  # take the average of samples, backprop\n",
    "            finish_episode(policy_loss_sum, encoder_loss_sum, baseline_value_batch)\n",
    "            del policy_loss_sum[:], encoder_loss_sum[:], baseline_value_batch[:]\n",
    "\n",
    "        # log and print out info     \n",
    "        if (index + 1) % 32 == 0:\n",
    "            stats = calculate_stats_from_cm(cm)\n",
    "            cm = np.zeros((LABEL_DIM, LABEL_DIM))\n",
    "            acc = stats[\"accuracy\"]\n",
    "            recall = stats[\"recall\"]\n",
    "            precision = stats[\"precision\"]\n",
    "            f1 = stats[\"f1\"]\n",
    "            writer.add_scalar(\"train_accuracy\", acc, len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_recall\", recall,  len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_precision\", precision,  len(train_loader)*epoch + index)\n",
    "            writer.add_scalar(\"train_f1\", f1,  len(train_loader)*epoch + index)\n",
    "            pbar.set_description(f\"episode: {index + 1}, reread_or_skim_times: {reread_or_skim_times}, accuracy: {acc:.3f}, precision: {precision:.3f}, recall: {recall:.3f}, f1: {f1:.2f}\")\n",
    "            \n",
    "            \"\"\"print(f'\\n current episode: {index + 1}')\n",
    "            # log the current position of the text which the agent has gone through\n",
    "            print('curr_step: ', curr_step)\n",
    "            # log the sum of the rereading and skimming times\n",
    "            print(f'current reread_or_skim_times: {reread_or_skim_times}')\"\"\"\n",
    "\n",
    "\n",
    "    print('Epoch time elapsed: %.2f s' % (time.time() - start))\n",
    "    print('reread_or_skim_times in this epoch:', reread_or_skim_times)\n",
    "    count_all, count_correct = evaluate_transformer(trnsxl, policy_s, policy_n, policy_c, valid_loader)\n",
    "    print('Epoch: %s, Accuracy on the validation set: %.2f' % (epoch + 1, count_correct / count_all))\n",
    "    writer.add_scalar(\"validation_acccuracy\", count_correct / count_all,  len(train_loader)*epoch + index)\n",
    "    # count_all, count_correct = evaluate(clstm, policy_s, policy_n, policy_c, train_loader)\n",
    "    # print('Epoch: %s, Accuracy on the training set: %.2f' % (epoch + 1, count_correct / count_all))\n",
    "    \n",
    "print('Compute the accuracy on the testing set...')\n",
    "count_all, count_correct = evaluate_transformer(trnsxl, policy_s, policy_n, policy_c, test_loader)\n",
    "print('Accuracy on the testing set: %.2f' % (count_correct / count_all))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...:   0%|          | 0/5000 [00:00<?, ?it/s]c:\\Users\\mrbal\\anaconda3\\envs\\blaze\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Convolution.cpp:1004.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "Evaluating...: 100%|██████████| 5000/5000 [01:25<00:00, 58.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time elapsed: 85.20 s\n",
      "Average FLOPs per sample:  2741607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_all, count_correct = evaluate_transformer(trnsxl, policy_s, policy_n, policy_c, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5022"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_correct / count_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "now_time = str(now.day) + \"_\" + str(now.month) + \"_\" + str(now.year) + \"_\" + str(now.hour) + \"_\" + str(now.minute)\n",
    "now_date = str(now.day) + \"_\" + str(now.month) + \"_\" + str(now.year)\n",
    "now_date, now_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_models(time_str, date_str, path: str = \".\"):\n",
    "    os.makedirs(f\"saved_models\\\\{date_str}\", exist_ok=True)\n",
    "    torch.save(clstm, f\"{date_str}\\\\clstm_{time_str}.pth\")\n",
    "    torch.save(policy_s, f\"{date_str}\\\\policy_s_{time_str}.pth\")\n",
    "    torch.save(policy_n, f\"{date_str}\\\\policy_n_{time_str}.pth\")\n",
    "    torch.save(policy_c, f\"{date_str}\\\\policy_c_{time_str}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models(now_time, now_date, \"saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clstm.eval()\n",
    "policy_s.eval()\n",
    "policy_n.eval()\n",
    "policy_c.eval()\n",
    "action_logs = []\n",
    "seen_logs = []\n",
    "writer = SummaryWriter()\n",
    "for i, (x, y) in enumerate(valid_loader):\n",
    "    print(i)\n",
    "    print(valid_data[\"text\"].iloc[i])\n",
    "    print(y)\n",
    "    action_log_batch = []\n",
    "    seen_batch = []\n",
    "    label = y.to(device).long() # for cross entropy loss, the long type is required\n",
    "    text = x.to(device).view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE) # transform 1*400 to 20*1*20\n",
    "    curr_step = 0\n",
    "    n_rnn_layers = clstm.n_rnn_layers\n",
    "    lstm_hidden_dim = clstm.lstm_hidden_dim\n",
    "    h_0 = torch.zeros([n_rnn_layers,1,lstm_hidden_dim]).to(device)\n",
    "    c_0 = torch.zeros([n_rnn_layers,1,lstm_hidden_dim]).to(device)\n",
    "    count = 0\n",
    "    while curr_step < 20 and count < 5: # loop until a text can be classified or currstep is up to 20\n",
    "        count += 1\n",
    "        # pass the input through cnn-lstm and policy s\n",
    "        text_input = text[curr_step] # text_input 1*20\n",
    "        text_str = train_data[\"text\"].iloc[i]\n",
    "        seen_batch.append(text_str.split()[curr_step * 20: (curr_step+1)*20])\n",
    "        ht, ct = clstm(text_input, h_0, c_0)  # 1 * 128\n",
    "        # if count == 1 and i == 0:\n",
    "        #     writer.add_graph(clstm, [text_input, h_0, c_0], verbose=True)\n",
    "        h_0 = ht.unsqueeze(0) # NUM_RNN_LAYERS * 1 * LSTM_HIDDEN_DIM, next input of lstm\n",
    "        c_0 = ct\n",
    "        # ht_ = ht.view(1, ht.shape[0] * ht.shape[2])\n",
    "        # draw a stop decision\n",
    "        stop_decision, log_prob_s = sample_policy_s(ht, policy_s)\n",
    "        # if count == 1 and i == 1:\n",
    "        #     writer.add_graph(policy_s, ht)\n",
    "        stop_decision = stop_decision.item()\n",
    "        if stop_decision == 1: # classify\n",
    "            break\n",
    "        else:\n",
    "            # draw an action (reread or skip)\n",
    "            step, log_prob_n = sample_policy_n(ht, policy_n)\n",
    "            # if count == 1 and i == 2:\n",
    "            #     writer.add_graph(policy_n, ht)\n",
    "            curr_step += int(step)  # reread or skip\n",
    "            action_log_batch.append({\"skip/reread\": step})\n",
    "    # draw a predicted label\n",
    "    output_c = policy_c(ht)\n",
    "    if i == 3:\n",
    "        writer.add_graph(policy_c, ht, verbose=True)\n",
    "        \n",
    "    # draw a predicted label \n",
    "    pred_label, log_prob_c = sample_policy_c(output_c)\n",
    "    action_log_batch.append({\"prediction\": pred_label, \"real\": label})\n",
    "    if pred_label.item() == label:\n",
    "        count_correct += 1\n",
    "    count_all += 1\n",
    "    action_logs.append(action_log_batch)\n",
    "    seen_logs.append(seen_batch)\n",
    "    if i == 10:\n",
    "        break\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_logs[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 3\n",
    "action_logs[ix], \" \".join(train_data[\"text\"].iloc[ix].split()[0:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(seen_logs[ix][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_correct / count_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
